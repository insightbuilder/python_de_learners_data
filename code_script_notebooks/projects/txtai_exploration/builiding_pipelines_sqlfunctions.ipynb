{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kvvihmmJa4c-"
      },
      "outputs": [],
      "source": [
        "!pip -q install txtai[all] langchain pypdf > /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from txtai.embeddings import Embeddings\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "import glob"
      ],
      "metadata": {
        "id": "fH4H2mk9bKdB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_text = []\n",
        "for pdf in glob.glob('/content/studies/*.pdf'):\n",
        "  loader = PyPDFLoader(pdf)\n",
        "  pages = loader.load()\n",
        "  pdf_text.extend(pages)"
      ],
      "metadata": {
        "id": "WOxG-Yk8bKpb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_text[0]"
      ],
      "metadata": {
        "id": "OXaOMVlbdTE3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "document_splitter = RecursiveCharacterTextSplitter(chunk_size=350,\n",
        "                                                   chunk_overlap=25,\n",
        "                                                   length_function=len)"
      ],
      "metadata": {
        "id": "GNZjPSfodTBw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_data = []\n",
        "for docs in pdf_text:\n",
        "  print(docs)\n",
        "  temp_split = document_splitter.split_text(docs.page_content)\n",
        "  split_data.extend(temp_split)"
      ],
      "metadata": {
        "id": "rOtX8eJCdS20"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create embeddings index with content enabled. The default behavior is to only store indexed vectors.\n",
        "embeddings = Embeddings({\"path\": \"sentence-transformers/nli-mpnet-base-v2\", \n",
        "                         \"content\": True, \n",
        "                         \"objects\": True})"
      ],
      "metadata": {
        "id": "WpVyMy51bKwN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an index for the list of text\n",
        "embeddings.index([(uid, \n",
        "                   text, \n",
        "                   None) for uid, text in enumerate(split_data)])"
      ],
      "metadata": {
        "id": "kGTzfLp6bK3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#embeddings.load(\"index\")"
      ],
      "metadata": {
        "id": "5dUA3uplgys6"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings.search(\"What is self ask?\",7)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ef81hpEAfqI1",
        "outputId": "601a6bdc-89fe-4bbc-c19b-2009a79d2d2d"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'id': '634',\n",
              "  'text': 'Right Answer\\nSelf-ask\\nChain of thought\\nQuestion When was the ﬁrst location of the world’s largest coffeehouse chain opened?\\nRight Answer\\nSelf-ask\\nChain of thought\\nQuestion Who directed the highest grossing ﬁlm?\\nRight Answer\\nSelf-ask\\nChain of thought\\n24',\n",
              "  'score': 0.5150313973426819},\n",
              " {'id': '447',\n",
              "  'text': '001\\n002\\n−0.04−0.02 0.00 0.02 0.04−0.04−0.020.000.020.04\\nSelf-ask\\nAnswered CorrectlyBoth Subquestions\\nAnswered CorrectlyFigure 4: Self-ask is able to narrow and sometimes close the compositionality gap on CC. Here,\\nself-ask uses a 1-shot prompt. Chain of thought performed within 1% of self-ask on this dataset and',\n",
              "  'score': 0.4897139072418213},\n",
              " {'id': '631',\n",
              "  'text': 'Self-Ask (ours) 30.0 36.1 35.4 13.8 27.0 16.2\\nSelf-Ask + Search Engine (ours) 40.1 52.6 53.1 15.2 27.2 19.6\\nTable 14 shows examples from Bamboogle where chain of thought outputs a full-sentence ﬁnal\\nanswer instead of a short form answer. The prompt for chain of thought demonstrates that the ﬁnal',\n",
              "  'score': 0.46568864583969116},\n",
              " {'id': '623',\n",
              "  'text': 'Table 12: Self-ask prompt for 2WikiMultiHopQA\\nQuestion: Who lived longer, Theodor Haecker or Harry Vaughan Watkins?\\nAre follow up questions needed here: Yes.\\nFollow up: How old was Theodor Haecker when he died?\\nIntermediate answer: Theodor Haecker was 65 years old when he died.\\nFollow up: How old was Harry Vaughan Watkins when he died?',\n",
              "  'score': 0.4637455940246582},\n",
              " {'id': '617',\n",
              "  'text': 'Table 10: Direct Prompt for 2WikiMultiHopQA\\nQuestion: Who lived longer, Theodor Haecker or Harry Vaughan Watkins?\\nSo the ﬁnal answer is: Harry Vaughan Watkins.\\nQuestion: Why did the founder of Versus die?\\nSo the ﬁnal answer is: Shot.\\nQuestion: Who is the grandchild of Dambar Shah?\\nSo the ﬁnal answer is: Rudra Shah.',\n",
              "  'score': 0.4594254791736603},\n",
              " {'id': '462',\n",
              "  'text': 'engine. The response is inserted back into the rest of the prompt to let the LM generate the next\\nfollow-up question. This process then repeats until the LM decides to output the ﬁnal answer.\\n3.3 I MPROVING SELF-ASK WITH A SEARCH ENGINE\\nUnlike chain of thought, self-ask clearly demarcates the beginning and end of every sub-question.',\n",
              "  'score': 0.4481210708618164},\n",
              " {'id': '444',\n",
              "  'text': 'Self-ask (depicted in Figure 3) requires a one- or few-shot prompt that demonstrates how to answer\\nthe questions. Our prompt starts with those examples, after which we append the inference-time\\nquestion. We then insert the phrase “Are follow up questions needed here:” at the end of the prompt',\n",
              "  'score': 0.44651538133621216}]"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Similar Clause\n",
        "\n",
        "similar(\"query\",\"num of candidates\")"
      ],
      "metadata": {
        "id": "75BRPSInckf7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings.search(\"select * from authors where similar('what is self ask')\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7CUmIJd8k-0b",
        "outputId": "1a74b013-2cb8-4564-87b5-97f27110a802"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'id': '444',\n",
              "  'text': 'Self-ask (depicted in Figure 3) requires a one- or few-shot prompt that demonstrates how to answer\\nthe questions. Our prompt starts with those examples, after which we append the inference-time\\nquestion. We then insert the phrase “Are follow up questions needed here:” at the end of the prompt',\n",
              "  'score': 0.5317630171775818},\n",
              " {'id': '60',\n",
              "  'text': 'over two or more Wikipedia passages, and (2) FEVER (Thorne et al., 2018), a fact veriﬁcation\\nbenchmark where each claim is annotated SUPPORTS, REFUTES, or NOT ENOUGH INFO, based\\non if there exists a Wikipedia passage to verify the claim. In this work, we operate in a question-only',\n",
              "  'score': 0.5158118009567261},\n",
              " {'id': '439',\n",
              "  'text': 'elicitive prompts can occasionally answer even more compositional questions than direct prompts\\ncorrectly answer sub-questions for, separately. This might be because elicitive prompts contain\\nmore information than direct ones. Note that the rest of this section shows that elicitive prompts',\n",
              "  'score': 0.5150432586669922}]"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Remember that input documents take the form of (id, data, tags) tuples. "
      ],
      "metadata": {
        "id": "T7VozKlxc3Dv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings.search(\"\"\"SELECT text, flag, entry \n",
        "                    FROM txtai \n",
        "                    WHERE similar('Language models') \n",
        "                    AND entry >= '2023-04-20'\"\"\")"
      ],
      "metadata": {
        "id": "4z0WmG1Nk-u_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3b1fa8a-e47b-4991-cff8-8cba4a2d2554"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'text': '2{jeffreyzhao,dianyu,dunan,izhak,yuancao}@google.com\\nABSTRACT\\nWhile large language models (LLMs) have demonstrated impressive performance\\nacross tasks in language understanding and interactive decision making, their\\nabilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action',\n",
              "  'flag': None,\n",
              "  'entry': '2023-04-21 10:26:59.312376'},\n",
              " {'text': 'However, as the language space Lis unlimited, learning in this augmented action space is difﬁcult\\nand requires strong language priors. In this paper, we mainly focus on the setup where a frozen\\nlarge language model, PaLM-540B (Chowdhery et al., 2022)1, is prompted with few-shot in-context',\n",
              "  'flag': None,\n",
              "  'entry': '2023-04-21 10:26:59.312376'},\n",
              " {'text': 'com/Authors-Notes/sparrow/sparrow-final.pdf .\\nEhsan Hosseini-Asl, Bryan McCann, Chien-Sheng Wu, Semih Yavuz, and Richard Socher. A simple\\nlanguage model for task-oriented dialogue. Advances in Neural Information Processing Systems ,\\n33:20179–20191, 2020.\\nWenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. Language models as zero-shot',\n",
              "  'flag': None,\n",
              "  'entry': '2023-04-21 10:26:59.312376'}]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings.search(\"\"\"SELECT length(text) FROM txtai \n",
        "                    WHERE similar('language models')\n",
        "                    AND score >= 0.15\"\"\")"
      ],
      "metadata": {
        "id": "Cs-jJRc6k-kk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f51162d-c95d-44fa-bb5d-de91b8cbe614"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'length(text)': 297}, {'length(text)': 289}, {'length(text)': 347}]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introducing Translation Pipeline"
      ],
      "metadata": {
        "id": "M9L6Qh_Gik_H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from txtai.pipeline import Translation\n",
        "\n",
        "# Translation pipeline\n",
        "translate = Translation()\n",
        "\n",
        "# Create embeddings index\n",
        "embeddings_translate = Embeddings({\"path\": \"sentence-transformers/nli-mpnet-base-v2\",\n",
        "                         \"content\": True,\n",
        "                         \"functions\": [translate]})\n"
      ],
      "metadata": {
        "id": "H2cJq9wAeN2O"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings_translate.index([(uid,\n",
        "                             text,\n",
        "                             None) for uid, text in enumerate(split_data)])"
      ],
      "metadata": {
        "id": "VfoRaD0nhIMD"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"\"\"\n",
        "select\n",
        "  text,\n",
        "  translation(text, 'de') 'text (DE)',\n",
        "  translation(text, 'es') 'text (ES)',\n",
        "  translation(text, 'fr') 'text (FR)'\n",
        "from txtai where similar('What is Language Model')\n",
        "limit 1\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "60Td-Xx-eNnj"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run a search using a custom SQL function\n",
        "embeddings_translate.search(query)"
      ],
      "metadata": {
        "id": "DIt2-KbmglPu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introducing Extractor Pipeline\n"
      ],
      "metadata": {
        "id": "jf3BMoU7iomI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "%%capture\n",
        "\n",
        "from txtai.embeddings import Embeddings\n",
        "from txtai.pipeline import Extractor\n",
        "\n",
        "# Create extractor instance\n",
        "extractor = Extractor(embeddings_translate, \"google/flan-t5-base\")"
      ],
      "metadata": {
        "id": "POh5rkTRiti9"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prompt(question):\n",
        "  return f\"\"\"Answer the following question using only the context below. Say 'no answer' when the question can't be answered.\n",
        "Question: {question}\n",
        "Context: \"\"\"\n",
        "\n",
        "def search(query, question=None):\n",
        "  # Default question to query if empty\n",
        "  if not question:\n",
        "    question = query\n",
        "\n",
        "  return extractor([(\"answer\", query, prompt(question), False)])[0][1]"
      ],
      "metadata": {
        "id": "4_YUvowHi9Nc"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What is self-ask algorithm?\"\n",
        "\n",
        "answer = search(question)\n",
        "\n",
        "print(question, answer)"
      ],
      "metadata": {
        "id": "ZWFRtXSHjc1z",
        "outputId": "5308c9c2-bb1c-495d-bedc-66b75f7cdbcb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What is self-ask algorithm? structured prompting lets us easily plug in a search engine to answer the follow-up questions\n"
          ]
        }
      ]
    }
  ]
}