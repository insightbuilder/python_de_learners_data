0:00 Intro Ollama
0:55 Why Ollama DSPy
1:40 Researching Ollam Installation
2:45 Ollama Model Library
4:30 Starting Ollama Install 
9:25 Installing Ollama-python
10:40 Starting Ollama Service
14:00 Installing Gemma-2B model
15:05 Generating with Gemma Model
15:55 Exploring Ollam Show Options 
19:45 Rerunning Ollama  
21:05 Troubleshooting ollama server
31:50 Editing Ollama.service file
26:30 Installing Ollama Python library
27:40 Using Ollama in Python 
29:10 Setting Ollama Client
30:15 Chatting with Gemma Model in Ollama
33:30 Using Generate Method in Ollama
36:40 Recap
38:25 Outro

Serve LLM From Your Local Machines With Ollama : Inferencing Open Source Gemma Model on Ollama

Ollama makes it super easy to load LLMs locally, run inference and even serve the model over the 
RestAPI servers in single commands. Video introduces the Ollama app installation on Linux, following 
that troubleshooting the app start in linux, downloading the gemma model, trouble shooting the server 
interfaces. Following that, uses Ollama-python library to connect with the models in the server and 
generate completions.

Model quantising & Serving:
- Complete preparing and serving huggingface models 
    a) Get the Lllama cpp install (Working)
        - git clone git@github.com:ollama/ollama.git ollama
            cd ollama
        - git submodule init
            git submodule update llm/llama.cpp
        - python3 -m venv llm/llama.cpp/.venv
            source llm/llama.cpp/.venv/bin/activate
            pip install -r llm/llama.cpp/requirements.txt
        - make -C llm/llama.cpp quantize
    a1) show the model download of starcoder in jupyter
    b) Use the models that has been downloaded to cache folder
        python llm/llama.cpp/convert.py /your/home/.cache/huggingface/hub/models--your_model/snapshots/ \
         --outtype f16 --outfile model_converted.bin
    c) Complete quantisation.
        - try directly work on hf cache folder in the .cache (works)
        llm/llama.cpp/quantize /your/home/.cache/huggingface/hub/models--your_model/model_converted.bin quantized.bin q4_0
    d) Make and Serve the model
        - Create the makeFile vim ../models--mistralai--your_model/yourModel
        - ollama create model_q4 -f yourModel
        - ollama run model_q4 "what is big bang theory"

Serving custom model to Ollama
0:00 Overview Intro
2:30 Reviewing Steps Followed
3:20 Downloading Starcoder 7B
7:35 Installing llamacpp modules
14:00 Locating Starcoder Model Path 
16:00 Converting Starcoder Model
18:00 Recovering From Errors
19:10 Starcoder Conversion fails
19:45 Converting Mistral Model
22:30 Quantizing Mistral Model
24:30 Creating Ollama ModelFile
26:30 Loading Ollama NameSpace
28:00 Using new_mistralq4 Ollama model
30:50 Recap
32:20 Linux Challenge & Objective
33:00 Outro