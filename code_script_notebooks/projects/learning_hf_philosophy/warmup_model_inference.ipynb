{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acf34a22-c1c2-4eb5-a2a7-328924a5d2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict, Dataset, load_from_disk\n",
    "from rich import print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42f52b77-3cec-40cb-bf19-a8742156d5b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset_name = \"Hypersniper/riddles_v1\"\n",
    "dataset= load_dataset(dataset_name, split='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96cf2457-7bfb-4ba9-be3a-f9de24e6ec42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['answer', 'instruction', 'output'],\n",
       "    num_rows: 469\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee9fd666-38c1-4c8f-ac8e-b0ef4f9e6218",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': 'music',\n",
       " 'instruction': 'At the sound of me, one may dream or stamp their feet, At the sound of me, one may laugh or sometimes weep.',\n",
       " 'output': \"The question seems to be a riddle that is focusing on an entity that can evoke a range of emotional responses. The given clues are that it can make someone dream or stamp their feet, and it can also make someone laugh or weep. \\n\\nLet's break down these clues. Dreaming and stamping feet are both reactions that can be associated with feelings of joy, excitement, or anticipation. On the other hand, laughing and weeping are expressions of happiness and sadness respectively. So, we are looking for something that can induce these varied emotional reactions.\\n\\nConsidering these clues, one plausible answer could be 'music'. Here's why: \\n\\nMusic has a profound impact on our emotions. It has the power to uplift our spirits, soothe our nerves, make us feel happy, sad, excited, calm, and even lead us into a state of introspection or dreaming. \\n\\nWhen we listen to upbeat music, it often makes us want to move or dance. Hence, the reference to 'stamp their feet'. In contrast, a soft, melodic tune can make us dreamy or reflective.\\n\\nMusic can also make us laugh or weep. A song with funny lyrics can make us laugh, while a melancholic melody or a song linked to personal memories can bring tears to our eyes.\\n\\nTherefore, considering all the clues in the riddle and the emotional responses they correspond to, 'music' seems to be a fitting answer.\"}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cceb0c55-0c88-4878-8b7b-23278ce28e83",
   "metadata": {},
   "source": [
    "Converting the riddles to multi-step conversation. \n",
    "\n",
    "> Work on following models that are already present locally\n",
    "\n",
    "     - roberta-large\n",
    "     - llama-2-7b chat\n",
    "     - codellama-7b-hf\n",
    "     \n",
    "> Ask each model to generate 10 more riddles by prompting with 10 riddles\n",
    "\n",
    "> Answer the synthetic riddles\n",
    "\n",
    "> Generate response for the follow up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a94b4de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2cab3035-76fb-41bd-bcde-9a6c31c297d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nroberta = AutoModelForCausalLM.from_pretrained(\\n    model1_path,\\n    device_map=\"auto\"\\n) '"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model1_path = '/home/kamal/.cache/huggingface/hub/models--roberta-large/snapshots/716877d372b884cad6d419d828bac6c85b3b18d9/'\n",
    "\n",
    "model_path=\"roberta-large\"\n",
    "\n",
    "\"\"\"\n",
    "roberta = AutoModelForCausalLM.from_pretrained(\n",
    "    model1_path,\n",
    "    device_map=\"auto\"\n",
    ") \"\"\" # takes around 2GB of VRAM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eabeee4-7eb6-4abf-a991-e45add40a5d2",
   "metadata": {},
   "source": [
    "BitsAndBytesConfig(\n",
    "     load_in_8bit=False,\n",
    "  \n",
    "      load_in_4bit=False,\n",
    " \n",
    "       llm_int8_threshold=6.0,\n",
    "\n",
    "        llm_int8_skip_modules=None,\n",
    "    \n",
    "    llm_int8_enable_fp32_cpu_offload=False,\n",
    "    \n",
    "    llm_int8_has_fp16_weight=False\n",
    "    ,\n",
    "    bnb_4bit_compute_dtype=Non\n",
    "    e,\n",
    "    bnb_4bit_quant_type='fp\n",
    "    4',\n",
    "    bnb_4bit_use_double_quant=Fa\n",
    "    lse,\n",
    "    **kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e62d82c9-84df-4312-a32b-a924a4adbd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f6f5a99-f847-49c3-9359-bd75296d0ae4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d69ca957b9d42af80fdbd8ba84e2264",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce9f34b449e2430eb5cbf02f6c408746",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n"
     ]
    }
   ],
   "source": [
    "roberta = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    device_map=\"auto\",\n",
    "    # torch_dtype=torch.bfloat16\n",
    ")  # takes 1.22 GB of VRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1bfd2c9-132c-4cd2-b9bc-ce42e6c1b273",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79fa735d1cbc46af85bee5154d5a1be9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "187c20d2eb4744bfb0884162ae1fa654",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dfe640aaf1641a8866ea972ae67f4bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d4f5243d5d84a11b186383517dcb5a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "roberta_tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "280bdde1-8562-4069-968f-f022fffda98c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "No chat template is defined for this tokenizer - using a default chat template that implements the ChatML format (without BOS/EOS tokens!). If the default is not appropriate for your model, please set `tokenizer.chat_template` to an appropriate template. See https://huggingface.co/docs/transformers/main/chat_templating for more information.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"{% for message in messages %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# there is default chat for the model, so it must give output\n",
    "roberta_tokenizer.default_chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c972a40-d284-4214-b37b-14ad0e1eda0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# get the questions\n",
    "questions = [q for q in dataset['instruction']]\n",
    "\n",
    "random.shuffle(questions)  # no need to reassign\n",
    "\n",
    "prompt_template = \"\"\"Below is a riddle. Come up with 10 more.\n",
    "Output just the riddles. No numbering and don't output anything else\"\"\"\n",
    "\n",
    "simple_request = \"Generate 10 riddles that you know\"\n",
    "\n",
    "prompt_riddles = prompt_template + \"\\n\\n\".join(questions[0:10])  \n",
    "\n",
    "# need to use join, list and string dont concatenate\n",
    "\n",
    "messages_to_model = [{\"role\":\"user\", \"content\":prompt_riddles}]\n",
    "\n",
    "simple_message = [{\"role\":\"user\", \"content\":simple_request}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1dff43e9-10d7-47ea-8384-a8d6c3ae95bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[41552, 15483,   757,  1215, 13124, 15483, 15698, 12105, 50118, 40025,\n",
       "           877,   158,   910, 40741,    14,    47,   216, 41552, 15483,   757,\n",
       "          1215,  1397, 15483, 15698, 50118]], device='cuda:0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# roberta_input = roberta_tokenizer.apply_chat_template(messages_to_roberta, return_tensors='pt').to('cuda')\n",
    "roberta_input = roberta_tokenizer.apply_chat_template(simple_message, \n",
    "                                                      return_tensors='pt').to('cuda')\n",
    "roberta_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e2d76cd3-c2da-4ffe-8975-bbdf197f5d98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>user\\nGenerate 10 riddles that you know<|im_end|>\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roberta_tokenizer.decode(roberta_input[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "708f972a-4ac4-4b85-a1aa-a0c2579df219",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerationConfig {\n",
       "  \"bos_token_id\": 0,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"pad_token_id\": 1\n",
       "}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roberta.generation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b4dee59a-9963-43f1-80ce-2e084b5a62e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta_output = roberta.generate(\n",
    "    roberta_input,\n",
    "    max_new_tokens=50,\n",
    "    do_sample=True,\n",
    "    pad_token_id=roberta_tokenizer.eos_token_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5b0ee9b6-54ca-4b5f-aa8d-0bca5ca97bb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2], device='cuda:0')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = roberta_output[0][len(roberta_input[0]):]\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "73ccce0e-f9fe-4497-87de-ea684957a9ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_text = roberta_tokenizer.decode(output, skip_special_tokens=True)\n",
    "output_text # There is no output at all. checking why, with a simpler prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f34b7a99-4491-4020-8928-699714745785",
   "metadata": {},
   "outputs": [],
   "source": [
    "del roberta  # removes the memory handles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ddb16ef4-15b6-4458-9f88-0ac0d5153e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()  # garbage collects the memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20e92f7-84f7-4f10-8d0e-e5a1ae5eef95",
   "metadata": {},
   "source": [
    "#### Working on llama2-7b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d8ae303e-50c9-4f81-a655-cc0653007b7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ceb2dbd733ba424babd2a14741a2e82c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# llama_path = \"/home/kamal/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-chat-hf/snapshots/c1b0db933684edbfe29a06fa47eb19cc48025e93/\"\n",
    "\n",
    "gemma_path = \"google/gemma-2b-it\"\n",
    "gemma = AutoModelForCausalLM.from_pretrained(\n",
    "    # pretrained_model_name_or_path='/home/aicoder/.cache/huggingface/hub/models--google--gemma-2b-it/snapshots/718cb189da9c5b2e55abe86f2eeffee9b4ae0dad/\n",
    "    gemma_path,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    local_files_only=True\n",
    ") # takes 11GB of VRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5c37fe38-1b2c-4983-ae46-032c66467788",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{% for message in messages %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\""
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gemma_tokenizer = AutoTokenizer.from_pretrained(gemma_path)\n",
    "# print(llama_tokenizer.default_chat_template) # rich's print fails due to tag\n",
    "gemma_tokenizer.default_chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7ac90ad4-ab13-4595-857b-8b982f4a6ae5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bos_token': '<bos>',\n",
       " 'eos_token': '<eos>',\n",
       " 'unk_token': '<unk>',\n",
       " 'pad_token': '<pad>',\n",
       " 'additional_special_tokens': ['<start_of_turn>', '<end_of_turn>']}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gemma_tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9a601624-37a0-43db-868a-c2db958d2758",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[     2,    106,   1645,    108,  38557, 235248, 235274, 235276, 193130,\n",
       "            674,    692,   1230,    107,    108]], device='cuda:0')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gemma_input = llama_tokenizer.apply_chat_template(simple_message,\n",
    "                                                  return_tensors='pt').to('cuda')\n",
    "gemma_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "35cb9f83-70a5-4103-939f-9897bc82ecbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "gemma_output = gemma.generate(\n",
    "    gemma_input,\n",
    "    max_new_tokens=100,\n",
    "    do_sample=True,\n",
    "    pad_token_id = llama_tokenizer.eos_token_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "937d4ebe-f79f-4459-9628-ea62468772ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'user\\nGenerate 10 riddles that you know\\n1. I have keys, but no doors.\\nI have space, but no room.\\nI have pages, but no words.\\n\\nWhat am I?\\n\\n\\n2. I have a bed, but I never sleep.\\nI have a mouth, but I never speak.\\nI have ears, but I never hear.\\n\\nWhat am I?\\n\\n\\n3. I have a tongue, but I never eat.\\nI have a head, but I never wear.\\nI have a'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# llama_output = llama_output[len(llama_input[0]):]\n",
    "# llama_output\n",
    "output = gemma_tokenizer.decode(gemma_output[0],skip_special_tokens=True)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "297e7798-23ef-4dea-a1ca-e4abeef329c7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[     2,    106,   1645,    108,  33501,    603,    476, 133326, 235265,\n",
       "          12542,    908,    675, 235248, 235274, 235276,    978, 235265,    108,\n",
       "           6140,   1317,    573, 193130, 235265,   1307,  96085,    578,   1453,\n",
       "         235303, 235251,   5033,   4341,   1354, 235285,  25469,    578,  10084,\n",
       "            578,  47331,   2003,    575,    861,   3142, 235265,    590,   1144,\n",
       "            793,  12100, 235269,    578,    590,   1453, 235303, 235251,   8044,\n",
       "          24306, 235265,    109,   1969,   2174,  11807,  28294, 235269,    665,\n",
       "            603,  24048,   1154,    476,  75735, 235265,   1165,    603,  10545,\n",
       "            675,    573,  25023, 235269,    578,    573,  25023,    603,  14471,\n",
       "         235265,    109, 159960, 235267,    685,    476,  12425,    575,    573,\n",
       "           5455,    576,   3354, 235269,  82056,    901,  90892,   1013,   2764,\n",
       "            476,  26911, 235265,  13227,  14987,    901,   3695,   8829, 235265,\n",
       "           2625,    970,   7888,   5604,  25201, 235269,    901,    674, 235303,\n",
       "         235256,   1861,    692,   2447,   1230,    682,    696,    832, 235265,\n",
       "            109, 235285,   1144,    476,   3967, 235303, 235256,   1963,   4034,\n",
       "         235265,   3194,   6181, 235269,    970,   2971,  49992,    675,   6785,\n",
       "         235265,   3194,  22121,    577,  11342, 235269,   1593,    970,  16482,\n",
       "            590,  10872, 235265,   5040,    692,    798,   4234,    573, 200967,\n",
       "          19312, 235265,    109, 235285,  66714,    611,    573,   6683, 235265,\n",
       "           1474,   9407,    611,    476,  61188, 235265,    109, 235280,   3741,\n",
       "           2346,  87980, 235269,  10240,    689,   2621, 235269,   3599,  13658,\n",
       "          29936,  13241,   2819, 235265, 235248,    109,   2169,    476,   3733,\n",
       "         235269,    590,   1144,   2145,   5985,    578,  10548, 235265,   2065,\n",
       "           1914,    682, 235269,    590,   3831,    476,   2040,    576,   5961,\n",
       "         235265,   2065,   1914,    682,   1653, 235269,    590,   1144,    573,\n",
       "           9670,    576,   5628, 235265,  76851,    682, 235269,    590,   3831,\n",
       "            573,  11988,    576,  59356, 235265,    109,   6571,    603,    693,\n",
       "            674,  10140,   2346,    476,   2590, 235265,   1474,    926,   3036,\n",
       "            611,    926,   1355, 235336,    109,   1841,    798,   6437,   4630,\n",
       "           3631,    578,   2001,   1174,    476,   1913,   1069, 235336,    109,\n",
       "           1841,    919, 235248, 235274, 235304,  16346,    901,    793,   1156,\n",
       "          29998, 235336,    107,    108]], device='cuda:0')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# working on getting the riddle based on the 10 riddles input\n",
    "gemma_10_input = gemma_tokenizer.apply_chat_template(messages_to_model,\n",
    "                                                     return_tensors='pt').to('cuda')\n",
    "gemma_10_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c93b98fc-2031-4d23-a5f5-1ee4c4d8ccec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<bos><start_of_turn>user\\nBelow is a riddle. Come up with 10 more.\\nOutput just the riddles. No numbering and don't output anything elseI bubble and laugh and spit water in your face. I am no lady, and I don't wear lace.\\n\\nAn open ended barrel, it is shaped like a hive. It is filled with the flesh, and the flesh is alive.\\n\\nStealthy as a shadow in the dead of night, cunning but affectionate if given a bite. Never owned but often loved. At my sport considered cruel, but that's because you never know me at all.\\n\\nI am a fire's best friend. When fat, my body fills with wind. When pushed to thin, through my nose I blow. Then you can watch the embers glow.\\n\\nI crawl on the earth. And rise on a pillar.\\n\\nA box without hinges, lock or key, yet golden treasure lies within. \\n\\nAs a whole, I am both safe and secure. Behead me, I become a place of meeting. Behead me again, I am the partner of ready. Restore me, I become the domain of beasts.\\n\\nWho is he that runs without a leg. And his house on his back?\\n\\nWhat can touch someone once and last them a life time?\\n\\nWhat has 13 hearts but no other organs?<end_of_turn>\\n\""
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gemma_tokenizer.decode(llama_10_input[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4b73d02a-4ca6-48d4-a5f3-2ce69a0fe673",
   "metadata": {},
   "outputs": [],
   "source": [
    "gemma_10_output = gemma.generate(\n",
    "    llama_10_input,\n",
    "    max_new_tokens=500,\n",
    "    do_sample=True,\n",
    "    pad_token_id=llama_tokenizer.eos_token_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f6f82462-d93c-4642-828c-7eca3cd2c0cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"user\\nBelow is a riddle. Come up with 10 more.\\nOutput just the riddles. No numbering and don't output anything elseI bubble and laugh and spit water in your face. I am no lady, and I don't wear lace.\\n\\nAn open ended barrel, it is shaped like a hive. It is filled with the flesh, and the flesh is alive.\\n\\nStealthy as a shadow in the dead of night, cunning but affectionate if given a bite. Never owned but often loved. At my sport considered cruel, but that's because you never know me at all.\\n\\nI am a fire's best friend. When fat, my body fills with wind. When pushed to thin, through my nose I blow. Then you can watch the embers glow.\\n\\nI crawl on the earth. And rise on a pillar.\\n\\nA box without hinges, lock or key, yet golden treasure lies within. \\n\\nAs a whole, I am both safe and secure. Behead me, I become a place of meeting. Behead me again, I am the partner of ready. Restore me, I become the domain of beasts.\\n\\nWho is he that runs without a leg. And his house on his back?\\n\\nWhat can touch someone once and last them a life time?\\n\\nWhat has 13 hearts but no other organs?\\nThe solution is a spider.\\n\\nHere are 10 riddles that follow the same theme:\\n\\n1. I have a nest, but I'm not a bird. I have silk, but I'm not a moth. I have wings, but I'm not an airplane. I hop, but I'm not a rabbit. I spin, but I'm not a spider. What am I?\\n\\n\\n2. I have a heart, but I'm not a human. I have a mouth, but I'm not a beast. I have a body, but I'm not a creature. I have a web, but I'm not a spider. What am I?\\n\\n\\n3. I have a mouth, but I'm not a human. I have a neck, but I'm not a beast. I have a body, but I'm not a creature. I have a shell, but I'm not a mollusc. What am I?\\n\\n\\n4. I have many eyes, but I'm not a creature. I have hairs, but I'm not a bird. I have a tail, but I'm not a reptile. What am I?\\n\\n\\n5. I have a nest, but I'm not a bird. I have silk, but I'm not a moth. I have wings, but I'm not an airplane. I hop, but I'm not a rabbit. I spin, but I'm not a spider. What am I?\\n\\n\\n6. I have a heart, but I'm not a human. I have a mouth, but I'm not a beast. I have a body, but I'm not a creature. I have a beard, but I'm not a man. What am I?\\n\\n\\n7. I have a mouth, but I'm not a human. I have a neck, but I'm not a beast. I have a body, but I'm not a creature. I have a beard, but I'm not a man. What am I?\\n\\n\\n8. I have a nest, but I'm not a bird. I have silk, but I'm not a moth. I have wings, but I'm not an airplane. I hop, but I'm not a rabbit. I spin, but I'm not a spider. What am I?\\n\\n\\n\""
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_10 = gemma_tokenizer.decode(gemma_10_output[0], skip_special_tokens=True)\n",
    "output_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a26e28d3-a303-47f3-9935-e56258002262",
   "metadata": {},
   "outputs": [],
   "source": [
    "del gemma\n",
    "torch.cuda.empty_cache()  # this time the model is not offloadin\n",
    "# restarting the server to release the memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34081f37-c363-40e6-aa75-8b5ab80a9fb2",
   "metadata": {},
   "source": [
    "### Llama observation\n",
    "\n",
    "- Llama provides the output of 10 riddles\n",
    "\n",
    "- Llama encoding and decoding is working same as roberta models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00d76358-9826-424a-97e6-f4ef06258f27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa7f9689dc15421895cfb4b75e85a743",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "code_llama_path = \"/home/kamal/.cache/huggingface/hub/models--codellama--CodeLlama-7b-hf/snapshots/bc5283229e2fe411552f55c71657e97edf79066c/\"\n",
    "code_tokenizer = AutoTokenizer.from_pretrained(code_llama_path)\n",
    "codellama = AutoModelForCausalLM.from_pretrained(\n",
    "    code_llama_path,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=quant_config,\n",
    "    torch_dtype=torch.bfloat16\n",
    ")  # takes around 11.5GB of VRAM\n",
    "# with 4-bit quantization 5GB of VRAM is consumed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d473eea-06c7-4ade-852b-51f32e5292a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% elif false == true and not '<<SYS>>' in messages[0]['content'] %}{% set loop_messages = messages %}{% set system_message = 'You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\\\n\\\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don\\\\'t know the answer to a question, please don\\\\'t share false information.' %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\\\n' + system_message + '\\\\n<</SYS>>\\\\n\\\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'system' %}{{ '<<SYS>>\\\\n' + content.strip() + '\\\\n<</SYS>>\\\\n\\\\n' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code_tokenizer.default_chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82d44f29-f0d5-4a9d-8ce5-98f97cd071e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_request = \"Generate 10 riddles that you know\"\n",
    "\n",
    "simple_message = [{\"role\":\"user\", \"content\":simple_request}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e4b07afe-d80b-454d-8c15-e21177e3b3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_request = \"Generate 10 dictionary related problems\"\n",
    "\n",
    "code_message = [{\"role\":\"user\", \"content\":code_request}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7942f2ed-26bb-4177-a280-92f3654d41f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using sep_token, but it is not set yet.\n",
      "Using pad_token, but it is not set yet.\n",
      "Using cls_token, but it is not set yet.\n",
      "Using mask_token, but it is not set yet.\n"
     ]
    }
   ],
   "source": [
    "code_input = code_tokenizer.apply_chat_template(simple_message,return_tensors='pt').to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c4182ad-3d8b-4545-baf2-39b1d7ed1d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using sep_token, but it is not set yet.\n",
      "Using pad_token, but it is not set yet.\n",
      "Using cls_token, but it is not set yet.\n",
      "Using mask_token, but it is not set yet.\n"
     ]
    }
   ],
   "source": [
    "code_input = code_tokenizer.apply_chat_template(code_message,return_tensors='pt').to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "541e2479-330a-4cfe-814f-b3703cd6a12d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "code_output = codellama.generate(\n",
    "    code_input,\n",
    "    max_new_tokens=500,\n",
    "    do_sample=True,\n",
    "    pad_token_id=code_tokenizer.eos_token_id\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "09804125-3a3c-4876-b81f-7ae4ca372c5d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "code_output = codellama.generate(\n",
    "    code_input,\n",
    "    max_new_tokens=500,\n",
    "    do_sample=True,\n",
    "    pad_token_id=code_tokenizer.eos_token_id,\n",
    "    temperature=0.2\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4996cfd8-5120-4d7c-a69e-ed6455099d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_output = codellama.generate(\n",
    "    code_input,\n",
    "    max_new_tokens=500,\n",
    "    # do_sample=True,\n",
    "    pad_token_id=code_tokenizer.eos_token_id,\n",
    "    # temperature=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e6318537-8343-4e1c-b218-25f04bd8c308",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_output = codellama.generate(\n",
    "    code_input,\n",
    "    max_new_tokens=500,\n",
    "    # do_sample=True,\n",
    "    pad_token_id=code_tokenizer.eos_token_id,\n",
    "    # temperature=0.2\n",
    "    repetition_penalty=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e7c754a9-97c9-4f93-9696-7da8d6a30f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_output = codellama.generate(\n",
    "    code_input,\n",
    "    max_new_tokens=500,\n",
    "    do_sample=True,\n",
    "    pad_token_id=code_tokenizer.eos_token_id,\n",
    "    # temperature=0.2\n",
    "    repetition_penalty=0.5,\n",
    "    top_p=10,\n",
    "    top_k=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "90d42456-3166-40ad-99e3-2a83cd6aa3e7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaSSLayaayaayaayaayaayaayaSSLayaSSLayaayaayaayaayaayaayaayaayaayaayaayaayaSSLayaayaayaayaayaayaayaayaayaayaSSLayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaSSLayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaSSLayaayaayaayaayaayaSSLayaayaayaayaayaayaayaayaSSLayaSSLayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaSSLayaayaayaayaayaayaSSLayaayaayaayaayaSSLayaayaayaayaayaayaayaayaayaSSLayaayaayaayaayaayaayaayaayaayaayaayaSSLayaayaayaayaayaayaayaSSLayaayaSSLayaSSLayaayaSSLayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaSSLayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaSSLayaayaayaSSLayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaSSLayaayaayaayaayaayaayaSSLayaayaayaayaayaayaayaayaayaSSLayaayaayaayaayaayaayaayaayaayaNaNayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaayaSSLSSLSSLayaSSLayaayaSSLayaayaayaayaaya'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = code_output[0][len(code_input[0]):]\n",
    "code_tokenizer.decode(output, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a3bfce-f1ae-4230-9a76-2c0746722370",
   "metadata": {},
   "source": [
    "### Code Llama observation\n",
    "\n",
    "- Model inference lead to OOM with 500 tokens request, when loaded with bfloat16\n",
    "\n",
    "- Model inference worked with 500 Tokens, with quant_config done with 4-bit.\n",
    "\n",
    "- In quantisation 1GB of Vram is consumed for inference\n",
    "\n",
    "\n",
    "- **The model out was gibberish**\n",
    "\n",
    "- Requested for the code related to dictionary\n",
    "\n",
    "- Even after reviewing the generation configs, only gibberish was generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61899424-60f5-40bb-ac35-d553dcd01869",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876e49f2-7b0c-4f65-b4e0-a4961036f588",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13522fe-47b0-4d4e-ad0e-c657161bd643",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925d4392-fe66-4331-9a4b-c7cc39424ee9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
