{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aicoder/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, list_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 6.87k/6.87k [00:00<00:00, 2.97MB/s]\n",
      "Downloading data: 100%|██████████| 91.8M/91.8M [00:26<00:00, 3.48MB/s]\n",
      "Downloading data: 100%|██████████| 15.8M/15.8M [00:04<00:00, 3.81MB/s]\n",
      "Downloading data: 100%|██████████| 6.12M/6.12M [00:01<00:00, 4.16MB/s]\n",
      "Generating train split: 100%|██████████| 18949/18949 [00:00<00:00, 45212.90 examples/s]\n",
      "Generating test split: 100%|██████████| 3269/3269 [00:00<00:00, 43053.42 examples/s]\n",
      "Generating ca_test split: 100%|██████████| 1237/1237 [00:00<00:00, 51929.24 examples/s]\n"
     ]
    }
   ],
   "source": [
    "billsum = load_dataset('billsum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aicoder/.local/lib/python3.10/site-packages/datasets/load.py:1454: FutureWarning: The repository for swag contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/swag\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "Downloading builder script: 100%|██████████| 7.97k/7.97k [00:00<00:00, 3.92MB/s]\n",
      "Downloading readme: 100%|██████████| 8.88k/8.88k [00:00<00:00, 16.0MB/s]\n",
      "Downloading data: 28.2MB [00:00, 30.0MB/s]                            \n",
      "Downloading data: 7.89MB [00:00, 18.2MB/s]                            \n",
      "Downloading data: 7.82MB [00:00, 25.8MB/s]                            \n",
      "Generating train split: 100%|██████████| 73546/73546 [00:01<00:00, 42951.61 examples/s]\n",
      "Generating validation split: 100%|██████████| 20006/20006 [00:00<00:00, 44280.92 examples/s]\n",
      "Generating test split: 100%|██████████| 20005/20005 [00:00<00:00, 42171.11 examples/s]\n"
     ]
    }
   ],
   "source": [
    "swag = load_dataset(\"swag\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 2.33k/2.33k [00:00<00:00, 1.01MB/s]\n",
      "Downloading data: 100%|██████████| 555k/555k [00:01<00:00, 508kB/s]\n",
      "Downloading data: 100%|██████████| 112k/112k [00:00<00:00, 236kB/s]\n",
      "Generating train split: 469 examples [00:00, 40424.33 examples/s]\n"
     ]
    }
   ],
   "source": [
    "riddles = load_dataset(\"Hypersniper/riddles_v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset arrow/g-ronimo___riddles_evolved to /home/kamal/.cache/huggingface/datasets/arrow/g-ronimo___riddles_evolved-9c7d39a649e0ad8e/0.0.0/74f69db2c14c2860059d39860b1f400a03d11bf7fb5a8258ca38c501c878c137...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f439430ee814b34b4702d4df7826f22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abddf9ffe4904a338c751a14d8325394",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset arrow downloaded and prepared to /home/kamal/.cache/huggingface/datasets/arrow/g-ronimo___riddles_evolved-9c7d39a649e0ad8e/0.0.0/74f69db2c14c2860059d39860b1f400a03d11bf7fb5a8258ca38c501c878c137. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08255a97c3a344f79a4d65931a421b04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "riddles_path =\"/home/kamal/.cache/huggingface/datasets/g-ronimo___riddles_evolved/\"\n",
    "cleaned_riddles = load_dataset(riddles_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# linchen\n",
    "sharegpt_path = \"/home/kamal/.cache/huggingface/datasets/Lin-Chen___json/\"\n",
    "sharegpt = load_dataset(path=\"Lin-Chen/ShareGPT4V\") # dataset errored at 2.85 GB... \n",
    "# sharegpt = load_dataset('share_gpt4_v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 665/665 [00:00<00:00, 440kB/s]\n",
      "Downloading data: 100%|██████████| 6.27M/6.27M [00:01<00:00, 3.66MB/s]\n",
      "Generating train split: 3243 examples [00:00, 91239.85 examples/s]\n"
     ]
    }
   ],
   "source": [
    "sharegpt_small = load_dataset(\"totally-not-an-llm/sharegpt-hyperfiltered-3k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 386/386 [00:00<00:00, 232kB/s]\n",
      "Downloading data: 100%|██████████| 40.3k/40.3k [00:00<00:00, 78.5kB/s]\n",
      "Downloading data: 100%|██████████| 66.3k/66.3k [00:00<00:00, 216kB/s]\n",
      "Downloading data: 100%|██████████| 30.2k/30.2k [00:00<00:00, 97.9kB/s]\n",
      "Downloading data: 100%|██████████| 22.0k/22.0k [00:00<00:00, 79.8kB/s]\n",
      "Downloading data: 100%|██████████| 52.2k/52.2k [00:00<00:00, 111kB/s]\n",
      "Downloading data: 100%|██████████| 44.0k/44.0k [00:00<00:00, 156kB/s]\n",
      "Downloading data: 100%|██████████| 7.77k/7.77k [00:00<00:00, 25.1kB/s]\n",
      "Downloading data: 100%|██████████| 64.5k/64.5k [00:00<00:00, 194kB/s]\n",
      "Downloading data: 100%|██████████| 20.3k/20.3k [00:00<00:00, 57.8kB/s]\n",
      "Generating train split: 0 examples [00:00, ? examples/s]\n"
     ]
    },
    {
     "ename": "DatasetGenerationError",
     "evalue": "An error occurred while generating the dataset",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/builder.py:1973\u001b[0m, in \u001b[0;36mArrowBasedBuilder._prepare_split_single\u001b[0;34m(self, gen_kwargs, fpath, file_format, max_shard_size, job_id)\u001b[0m\n\u001b[1;32m   1972\u001b[0m _time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m-> 1973\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, table \u001b[38;5;129;01min\u001b[39;00m generator:\n\u001b[1;32m   1974\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m max_shard_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m writer\u001b[38;5;241m.\u001b[39m_num_bytes \u001b[38;5;241m>\u001b[39m max_shard_size:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/packaged_modules/text/text.py:89\u001b[0m, in \u001b[0;36mText._generate_tables\u001b[0;34m(self, files)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m---> 89\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m batch:\n",
      "File \u001b[0;32m/usr/lib/python3.10/codecs.py:322\u001b[0m, in \u001b[0;36mBufferedIncrementalDecoder.decode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m    321\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer \u001b[38;5;241m+\u001b[39m \u001b[38;5;28minput\u001b[39m\n\u001b[0;32m--> 322\u001b[0m (result, consumed) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_buffer_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;66;03m# keep undecoded input until the next call\u001b[39;00m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0x92 in position 24045: invalid start byte",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mDatasetGenerationError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m philo_quotes \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmertbozkurt/quotes_philosophers\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/load.py:2574\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, token, use_auth_token, task, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001b[0m\n\u001b[1;32m   2571\u001b[0m try_from_hf_gcs \u001b[38;5;241m=\u001b[39m path \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m _PACKAGED_DATASETS_MODULES\n\u001b[1;32m   2573\u001b[0m \u001b[38;5;66;03m# Download and prepare data\u001b[39;00m\n\u001b[0;32m-> 2574\u001b[0m \u001b[43mbuilder_instance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2575\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2576\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2577\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2578\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtry_from_hf_gcs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtry_from_hf_gcs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2579\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2580\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2581\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2583\u001b[0m \u001b[38;5;66;03m# Build dataset for splits\u001b[39;00m\n\u001b[1;32m   2584\u001b[0m keep_in_memory \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2585\u001b[0m     keep_in_memory \u001b[38;5;28;01mif\u001b[39;00m keep_in_memory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m is_small_dataset(builder_instance\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdataset_size)\n\u001b[1;32m   2586\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/builder.py:1005\u001b[0m, in \u001b[0;36mDatasetBuilder.download_and_prepare\u001b[0;34m(self, output_dir, download_config, download_mode, verification_mode, ignore_verifications, try_from_hf_gcs, dl_manager, base_path, use_auth_token, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)\u001b[0m\n\u001b[1;32m   1003\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m num_proc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1004\u001b[0m         prepare_split_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_proc\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_proc\n\u001b[0;32m-> 1005\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_download_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1006\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdl_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1007\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1008\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mprepare_split_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1009\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdownload_and_prepare_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1010\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1011\u001b[0m \u001b[38;5;66;03m# Sync info\u001b[39;00m\n\u001b[1;32m   1012\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdataset_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(split\u001b[38;5;241m.\u001b[39mnum_bytes \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39msplits\u001b[38;5;241m.\u001b[39mvalues())\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/builder.py:1100\u001b[0m, in \u001b[0;36mDatasetBuilder._download_and_prepare\u001b[0;34m(self, dl_manager, verification_mode, **prepare_split_kwargs)\u001b[0m\n\u001b[1;32m   1096\u001b[0m split_dict\u001b[38;5;241m.\u001b[39madd(split_generator\u001b[38;5;241m.\u001b[39msplit_info)\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1099\u001b[0m     \u001b[38;5;66;03m# Prepare split will record examples associated to the split\u001b[39;00m\n\u001b[0;32m-> 1100\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mprepare_split_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1101\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1102\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[1;32m   1103\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot find data file. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1104\u001b[0m         \u001b[38;5;241m+\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmanual_download_instructions \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1105\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mOriginal error:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1106\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)\n\u001b[1;32m   1107\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/builder.py:1860\u001b[0m, in \u001b[0;36mArrowBasedBuilder._prepare_split\u001b[0;34m(self, split_generator, file_format, num_proc, max_shard_size)\u001b[0m\n\u001b[1;32m   1858\u001b[0m job_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   1859\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pbar:\n\u001b[0;32m-> 1860\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m job_id, done, content \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_split_single(\n\u001b[1;32m   1861\u001b[0m         gen_kwargs\u001b[38;5;241m=\u001b[39mgen_kwargs, job_id\u001b[38;5;241m=\u001b[39mjob_id, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_prepare_split_args\n\u001b[1;32m   1862\u001b[0m     ):\n\u001b[1;32m   1863\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[1;32m   1864\u001b[0m             result \u001b[38;5;241m=\u001b[39m content\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/builder.py:2016\u001b[0m, in \u001b[0;36mArrowBasedBuilder._prepare_split_single\u001b[0;34m(self, gen_kwargs, fpath, file_format, max_shard_size, job_id)\u001b[0m\n\u001b[1;32m   2014\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, DatasetGenerationError):\n\u001b[1;32m   2015\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 2016\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DatasetGenerationError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while generating the dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m   2018\u001b[0m \u001b[38;5;28;01myield\u001b[39;00m job_id, \u001b[38;5;28;01mTrue\u001b[39;00m, (total_num_examples, total_num_bytes, writer\u001b[38;5;241m.\u001b[39m_features, num_shards, shard_lengths)\n",
      "\u001b[0;31mDatasetGenerationError\u001b[0m: An error occurred while generating the dataset"
     ]
    }
   ],
   "source": [
    "philo_quotes = load_dataset(\"mertbozkurt/quotes_philosophers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 7.54M/7.54M [00:03<00:00, 2.42MB/s]\n",
      "Generating train split: 45576 examples [00:00, 1179007.74 examples/s]\n"
     ]
    }
   ],
   "source": [
    "motivational = load_dataset(\"leonweber/teaching_motivational_quotes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 28.0/28.0 [00:00<00:00, 17.4kB/s]\n",
      "Downloading data: 100%|██████████| 979k/979k [00:01<00:00, 807kB/s]\n",
      "Generating train split: 2845 examples [00:00, 258240.89 examples/s]\n"
     ]
    }
   ],
   "source": [
    "funny_quote= load_dataset(\"Khalida1w/funny_quotes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 3.41k/3.41k [00:00<00:00, 1.99MB/s]\n",
      "Downloading data: 100%|██████████| 300M/300M [02:42<00:00, 1.84MB/s] \n",
      "Generating train split: 1000000 examples [00:02, 426383.87 examples/s]\n"
     ]
    }
   ],
   "source": [
    "reddit_joke = load_dataset(\"SocialGrep/one-million-reddit-jokes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 6.06M/6.06M [00:02<00:00, 2.44MB/s]\n",
      "Downloading data: 100%|██████████| 347k/347k [00:00<00:00, 672kB/s]\n",
      "Downloading data: 100%|██████████| 335k/335k [00:01<00:00, 279kB/s]\n",
      "Generating train split: 100%|██████████| 14732/14732 [00:00<00:00, 431464.67 examples/s]\n",
      "Generating test split: 100%|██████████| 819/819 [00:00<00:00, 294356.04 examples/s]\n",
      "Generating validation split: 100%|██████████| 818/818 [00:00<00:00, 255639.72 examples/s]\n"
     ]
    }
   ],
   "source": [
    "samsum = load_dataset('samsum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18007/723396321.py:1: FutureWarning: list_datasets is deprecated and will be removed in the next major version of datasets. Use 'huggingface_hub.list_datasets' instead.\n",
      "  data_list = list_datasets()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "104008"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_list = list_datasets()\n",
    "len(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Name: acronym_identification, Tags: ['task_categories:token-classification', 'annotations_creators:expert-generated', 'language_creators:found', 'multilinguality:monolingual', 'size_categories:10K<n<100K', 'source_datasets:original', 'language:en', 'license:mit', 'acronym-identification', 'arxiv:2010.14678', 'region:us']\n"
     ]
    }
   ],
   "source": [
    "import huggingface_hub\n",
    "for f in huggingface_hub.list_datasets():\n",
    "    print(f)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aicoder/.local/lib/python3.10/site-packages/datasets/load.py:2516: FutureWarning: 'ignore_verifications' was deprecated in favor of 'verification_mode' in version 2.9.1 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'verification_mode=no_checks' instead.\n",
      "  warnings.warn(\n",
      "Downloading readme: 100%|██████████| 10.5k/10.5k [00:00<00:00, 7.41MB/s]\n",
      "Downloading data: 100%|██████████| 733k/733k [00:00<00:00, 1.11MB/s]\n",
      "Downloading data: 100%|██████████| 6.36M/6.36M [00:01<00:00, 4.90MB/s]\n",
      "Downloading data: 100%|██████████| 657k/657k [00:01<00:00, 510kB/s]\n",
      "Generating test split: 100%|██████████| 4358/4358 [00:00<00:00, 350793.11 examples/s]\n",
      "Generating train split: 100%|██████████| 36718/36718 [00:00<00:00, 626697.22 examples/s]\n",
      "Generating validation split: 100%|██████████| 3760/3760 [00:00<00:00, 335672.88 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# using wiki-text for causal and masked language modeling\n",
    "wiki_data = load_dataset(\"wikitext\",\n",
    "                         \"wikitext-2-raw-v1\",\n",
    "                         split='train',\n",
    "                         ignore_verifications=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 733k/733k [00:00<00:00, 1.42MB/s]\n",
      "Downloading data: 100%|██████████| 157M/157M [00:45<00:00, 3.44MB/s] \n",
      "Downloading data: 100%|██████████| 157M/157M [00:47<00:00, 3.34MB/s] \n",
      "Downloading data: 100%|██████████| 657k/657k [00:00<00:00, 1.48MB/s]\n",
      "Generating test split: 100%|██████████| 4358/4358 [00:00<00:00, 335532.00 examples/s]\n",
      "Generating train split: 100%|██████████| 1801350/1801350 [00:01<00:00, 1473252.61 examples/s]\n",
      "Generating validation split: 100%|██████████| 3760/3760 [00:00<00:00, 1642769.07 examples/s]\n"
     ]
    }
   ],
   "source": [
    "wiki_data_test = load_dataset(\"wikitext\",\n",
    "                              'wikitext-103-raw-v1',\n",
    "                              split=('test','validation'),\n",
    "                              verification_mode=\"no_checks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/kamal/jupyter_env/bin:/usr/local/sbin:/usr/sbin:/sbin:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games:/home/kamal/.dotnet/tools\n"
     ]
    }
   ],
   "source": [
    "!echo $PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export HF_DATASETS_CACHE=\"~/.cache/huggingface/datasets/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset arrow (/home/kamal/.cache/huggingface/datasets/arrow/wikitext-2-raw-v1-3f7e6bb1b926f47c/0.0.0/74f69db2c14c2860059d39860b1f400a03d11bf7fb5a8258ca38c501c878c137)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72ae7e28137b4f1898e0e3a4bd213225",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wikitext_2_raw = load_dataset(path=\"/home/kamal/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 36718\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 3760\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 4358\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikitext_2_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 35.3k/35.3k [00:00<00:00, 172kB/s]\n",
      "Downloading data: 100%|██████████| 649k/649k [00:00<00:00, 986kB/s]\n",
      "Downloading data: 100%|██████████| 75.7k/75.7k [00:00<00:00, 218kB/s]\n",
      "Downloading data: 100%|██████████| 308k/308k [00:00<00:00, 852kB/s]\n",
      "Generating train split: 100%|██████████| 3668/3668 [00:00<00:00, 598626.73 examples/s]\n",
      "Generating validation split: 100%|██████████| 408/408 [00:00<00:00, 129133.42 examples/s]\n",
      "Generating test split: 100%|██████████| 1725/1725 [00:00<00:00, 483731.66 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# glue mrpc is used for peft model training\n",
    "glue_mrpc = load_dataset(\"glue\", 'mrpc', \n",
    "                         cache_dir=\"~/.cache/huggingface/datasets/glue\",\n",
    "                        download_mode=\"reuse_dataset_if_exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset arrow/mrpc to /home/kamal/.cache/huggingface/datasets/arrow/mrpc-04ea723e68ea8cd6/0.0.0/74f69db2c14c2860059d39860b1f400a03d11bf7fb5a8258ca38c501c878c137...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "005cb685e55c47e4ac9745da64d0744e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42dc9b82042e429f9238a6294cdab783",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset arrow downloaded and prepared to /home/kamal/.cache/huggingface/datasets/arrow/mrpc-04ea723e68ea8cd6/0.0.0/74f69db2c14c2860059d39860b1f400a03d11bf7fb5a8258ca38c501c878c137. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed9419771e9347828b2245e6aadb1b04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# glue mrpc is used for peft model training\n",
    "glue_mrpc = load_dataset(path=\"/home/kamal/.cache/huggingface/datasets/glue/mrpc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset arrow/ax to /home/kamal/.cache/huggingface/datasets/arrow/ax-5835a4cb39407490/0.0.0/74f69db2c14c2860059d39860b1f400a03d11bf7fb5a8258ca38c501c878c137...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7ba5609f8f0486ea812b730312ecc4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1d60ba9ab4643d88952b81c4a2b96a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset arrow downloaded and prepared to /home/kamal/.cache/huggingface/datasets/arrow/ax-5835a4cb39407490/0.0.0/74f69db2c14c2860059d39860b1f400a03d11bf7fb5a8258ca38c501c878c137. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a0b57d3a2314da1ab079531bee94d21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# glue mrpc is used for peft model training\n",
    "glue_ax = load_dataset(path=\"/home/kamal/.cache/huggingface/datasets/glue/ax\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset arrow/sentences_allagree to /home/kamal/.cache/huggingface/datasets/arrow/sentences_allagree-0be007f7184eb234/0.0.0/74f69db2c14c2860059d39860b1f400a03d11bf7fb5a8258ca38c501c878c137...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "424847ee108341018effd496bb7133b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84549fafe2c34847ad471849b844cf90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset arrow downloaded and prepared to /home/kamal/.cache/huggingface/datasets/arrow/sentences_allagree-0be007f7184eb234/0.0.0/74f69db2c14c2860059d39860b1f400a03d11bf7fb5a8258ca38c501c878c137. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88a67f57c1b54d3eb04c6022f933eedd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fin_phrase_sentAll = load_dataset(path=\"/home/kamal/.cache/huggingface/datasets/financial_phrasebank/sentences_allagree\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence', 'label'],\n",
       "        num_rows: 2264\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fin_phrase_sentAll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# glue cola is used for text classification\n",
    "glue_cola = load_dataset(\"glue\", \"cola\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mnli_matched\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 1.21M/1.21M [00:02<00:00, 495kB/s]\n",
      "Downloading data: 100%|██████████| 1.22M/1.22M [00:02<00:00, 548kB/s]\n",
      "Generating validation split: 100%|██████████| 9815/9815 [00:00<00:00, 722128.36 examples/s]\n",
      "Generating test split: 100%|██████████| 9796/9796 [00:00<00:00, 1825295.51 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mnli_mismatched\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 1.25M/1.25M [00:02<00:00, 548kB/s]\n",
      "Downloading data: 100%|██████████| 1.26M/1.26M [00:02<00:00, 541kB/s]\n",
      "Generating validation split: 100%|██████████| 9832/9832 [00:00<00:00, 1805613.07 examples/s]\n",
      "Generating test split: 100%|██████████| 9847/9847 [00:00<00:00, 1435219.50 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mrpc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 649k/649k [00:00<00:00, 1.55MB/s]\n",
      "Downloading data: 100%|██████████| 75.7k/75.7k [00:00<00:00, 246kB/s]\n",
      "Downloading data: 100%|██████████| 308k/308k [00:00<00:00, 862kB/s]\n",
      "Generating train split: 100%|██████████| 3668/3668 [00:00<00:00, 355987.39 examples/s]\n",
      "Generating validation split: 100%|██████████| 408/408 [00:00<00:00, 92661.69 examples/s]\n",
      "Generating test split: 100%|██████████| 1725/1725 [00:00<00:00, 336801.71 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qnli\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 17.5M/17.5M [00:07<00:00, 2.26MB/s]\n",
      "Downloading data: 100%|██████████| 872k/872k [00:02<00:00, 377kB/s]\n",
      "Downloading data: 100%|██████████| 877k/877k [00:02<00:00, 399kB/s]\n",
      "Generating train split: 100%|██████████| 104743/104743 [00:00<00:00, 2531368.03 examples/s]\n",
      "Generating validation split: 100%|██████████| 5463/5463 [00:00<00:00, 2223099.13 examples/s]\n",
      "Generating test split: 100%|██████████| 5463/5463 [00:00<00:00, 2281764.86 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qqp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 33.6M/33.6M [00:18<00:00, 1.83MB/s]\n",
      "Downloading data: 100%|██████████| 3.73M/3.73M [00:06<00:00, 589kB/s]\n",
      "Downloading data: 100%|██████████| 36.7M/36.7M [00:25<00:00, 1.45MB/s]\n",
      "Generating train split: 100%|██████████| 363846/363846 [00:00<00:00, 4186698.01 examples/s]\n",
      "Generating validation split: 100%|██████████| 40430/40430 [00:00<00:00, 5421391.69 examples/s]\n",
      "Generating test split: 100%|██████████| 390965/390965 [00:00<00:00, 5613574.18 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rte\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 584k/584k [00:01<00:00, 493kB/s]\n",
      "Downloading data: 100%|██████████| 69.0k/69.0k [00:01<00:00, 52.2kB/s]\n",
      "Downloading data: 100%|██████████| 621k/621k [00:01<00:00, 529kB/s]\n",
      "Generating train split: 100%|██████████| 2490/2490 [00:00<00:00, 282112.83 examples/s]\n",
      "Generating validation split: 100%|██████████| 277/277 [00:00<00:00, 83560.29 examples/s]\n",
      "Generating test split: 100%|██████████| 3000/3000 [00:00<00:00, 449341.57 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sst2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 3.11M/3.11M [00:00<00:00, 3.68MB/s]\n",
      "Downloading data: 100%|██████████| 72.8k/72.8k [00:01<00:00, 45.8kB/s]\n",
      "Downloading data: 100%|██████████| 148k/148k [00:01<00:00, 87.8kB/s]\n",
      "Generating train split: 100%|██████████| 67349/67349 [00:00<00:00, 2200736.85 examples/s]\n",
      "Generating validation split: 100%|██████████| 872/872 [00:00<00:00, 427320.14 examples/s]\n",
      "Generating test split: 100%|██████████| 1821/1821 [00:00<00:00, 862057.29 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stsb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 502k/502k [00:03<00:00, 138kB/s]\n",
      "Downloading data: 100%|██████████| 151k/151k [00:01<00:00, 130kB/s]\n",
      "Downloading data: 100%|██████████| 114k/114k [00:01<00:00, 98.8kB/s]\n",
      "Generating train split: 100%|██████████| 5749/5749 [00:00<00:00, 833064.56 examples/s]\n",
      "Generating validation split: 100%|██████████| 1500/1500 [00:00<00:00, 823920.38 examples/s]\n",
      "Generating test split: 100%|██████████| 1379/1379 [00:00<00:00, 630196.69 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wnli\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 38.8k/38.8k [00:01<00:00, 28.1kB/s]\n",
      "Downloading data: 100%|██████████| 11.1k/11.1k [00:01<00:00, 9.32kB/s]\n",
      "Downloading data: 100%|██████████| 13.6k/13.6k [00:01<00:00, 10.9kB/s]\n",
      "Generating train split: 100%|██████████| 635/635 [00:00<00:00, 130060.70 examples/s]\n",
      "Generating validation split: 100%|██████████| 71/71 [00:00<00:00, 17652.38 examples/s]\n",
      "Generating test split: 100%|██████████| 146/146 [00:00<00:00, 27438.32 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# getting all the datasets to local for easier loading & working\n",
    "# Got all the datasets locally\n",
    "glue_tasks = ['ax', 'cola', 'mnli', 'mnli_matched',\n",
    "              'mnli_mismatched', 'mrpc', 'qnli',\n",
    "              'qqp', 'rte', 'sst2', 'stsb', 'wnli']\n",
    "# glue all tasks download\n",
    "for task in glue_tasks[3:]:\n",
    "    print(task)\n",
    "    temp = load_dataset('glue', task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 7.83k/7.83k [00:00<00:00, 3.38MB/s]\n",
      "Downloading data: 100%|██████████| 14.5M/14.5M [00:05<00:00, 2.59MB/s]\n",
      "Downloading data: 100%|██████████| 1.82M/1.82M [00:00<00:00, 2.77MB/s]\n",
      "Generating train split: 100%|██████████| 87599/87599 [00:00<00:00, 698282.76 examples/s]\n",
      "Generating validation split: 100%|██████████| 10570/10570 [00:00<00:00, 1088881.08 examples/s]\n",
      "Downloading readme: 100%|██████████| 8.18k/8.18k [00:00<00:00, 4.81MB/s]\n",
      "Downloading data: 100%|██████████| 16.4M/16.4M [00:07<00:00, 2.21MB/s]\n",
      "Downloading data: 100%|██████████| 1.35M/1.35M [00:03<00:00, 433kB/s]\n",
      "Generating train split: 100%|██████████| 130319/130319 [00:00<00:00, 1124274.45 examples/s]\n",
      "Generating validation split: 100%|██████████| 11873/11873 [00:00<00:00, 1253151.10 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# QnA training, got both the datasets\n",
    "squad_v1 = load_dataset(\"squad\", split='all')\n",
    "squad_v2 = load_dataset(\"squad_v2\", split='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 14.8M/14.8M [00:06<00:00, 2.13MB/s]\n",
      "Downloading data: 100%|██████████| 4.81M/4.81M [00:03<00:00, 1.59MB/s]\n",
      "Downloading data: 100%|██████████| 4.78M/4.78M [00:02<00:00, 1.62MB/s]\n",
      "Generating train split: 100%|██████████| 73546/73546 [00:00<00:00, 1696890.23 examples/s]\n",
      "Generating validation split: 100%|██████████| 20006/20006 [00:00<00:00, 2426583.16 examples/s]\n",
      "Generating test split: 100%|██████████| 20005/20005 [00:00<00:00, 2067949.51 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Multiple Choice\n",
    "swag = load_dataset(\"swag\", 'regular') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 1.23M/1.23M [00:02<00:00, 536kB/s]\n",
      "Downloading data: 100%|██████████| 312k/312k [00:00<00:00, 338kB/s]\n",
      "Downloading data: 100%|██████████| 283k/283k [00:00<00:00, 314kB/s]\n",
      "Generating train split: 100%|██████████| 14041/14041 [00:00<00:00, 427204.11 examples/s]\n",
      "Generating validation split: 100%|██████████| 3250/3250 [00:00<00:00, 277553.56 examples/s]\n",
      "Generating test split: 100%|██████████| 3453/3453 [00:00<00:00, 433737.59 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# NER dataset\n",
    "conll = load_dataset(\"conll2003\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 512/512 [00:00<00:00, 334kB/s]\n",
      "Downloading data: 100%|██████████| 31.0M/31.0M [00:09<00:00, 3.11MB/s]\n",
      "Downloading data: 100%|██████████| 1.61M/1.61M [00:03<00:00, 508kB/s]\n",
      "Generating train split: 12947 examples [00:00, 174527.89 examples/s]\n",
      "Generating test split: 690 examples [00:00, 145752.91 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# openAssistant Mistral FT dataset\n",
    "oasst_top1 = load_dataset(\"OpenAssistant/oasst_top1_2023-08-25\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 10.2k/10.2k [00:00<00:00, 6.42MB/s]\n",
      "Downloading data: 100%|██████████| 39.5M/39.5M [00:17<00:00, 2.21MB/s]\n",
      "Downloading data: 100%|██████████| 2.08M/2.08M [00:02<00:00, 890kB/s]\n",
      "Generating train split: 100%|██████████| 84437/84437 [00:00<00:00, 563947.39 examples/s]\n",
      "Generating validation split: 100%|██████████| 4401/4401 [00:00<00:00, 547667.46 examples/s]\n",
      "Downloading readme: 100%|██████████| 10.6k/10.6k [00:00<00:00, 5.68MB/s]\n",
      "Downloading data: 100%|██████████| 63.5M/63.5M [00:25<00:00, 2.47MB/s]\n",
      "Downloading data: 100%|██████████| 3.18M/3.18M [00:01<00:00, 1.91MB/s]\n",
      "Generating train split: 100%|██████████| 128575/128575 [00:00<00:00, 420505.74 examples/s]\n",
      "Generating validation split: 100%|██████████| 6599/6599 [00:00<00:00, 371699.24 examples/s]\n"
     ]
    }
   ],
   "source": [
    "oasst_1 = load_dataset(\"OpenAssistant/oasst1\")\n",
    "oasst_2 = load_dataset(\"OpenAssistant/oasst2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 23.9k/23.9k [00:00<00:00, 9.73MB/s]\n",
      "Downloading data: 100%|██████████| 2.61M/2.61M [00:03<00:00, 744kB/s]\n",
      "Downloading data: 100%|██████████| 3.05M/3.05M [00:03<00:00, 869kB/s]\n",
      "Downloading data: 100%|██████████| 282k/282k [00:02<00:00, 139kB/s]\n",
      "Generating train split: 100%|██████████| 45000/45000 [00:00<00:00, 2396532.12 examples/s]\n",
      "Generating test split: 100%|██████████| 50000/50000 [00:00<00:00, 5192769.77 examples/s]\n",
      "Generating validation split: 100%|██████████| 5000/5000 [00:00<00:00, 2847070.32 examples/s]\n",
      "Downloading data: 100%|██████████| 233k/233k [00:01<00:00, 230kB/s]\n",
      "Downloading data: 100%|██████████| 105k/105k [00:00<00:00, 202kB/s]\n",
      "Downloading data: 100%|██████████| 28.6k/28.6k [00:00<00:00, 61.6kB/s]\n",
      "Generating train split: 100%|██████████| 3257/3257 [00:00<00:00, 582775.83 examples/s]\n",
      "Generating test split: 100%|██████████| 1421/1421 [00:00<00:00, 254433.55 examples/s]\n",
      "Generating validation split: 100%|██████████| 374/374 [00:00<00:00, 142944.20 examples/s]\n",
      "Downloading data: 100%|██████████| 816k/816k [00:02<00:00, 350kB/s]\n",
      "Downloading data: 100%|██████████| 278k/278k [00:01<00:00, 149kB/s]\n",
      "Downloading data: 100%|██████████| 103k/103k [00:01<00:00, 64.9kB/s]\n",
      "Generating train split: 100%|██████████| 9000/9000 [00:00<00:00, 2535343.95 examples/s]\n",
      "Generating test split: 100%|██████████| 2970/2970 [00:00<00:00, 1416865.66 examples/s]\n",
      "Generating validation split: 100%|██████████| 1000/1000 [00:00<00:00, 820803.13 examples/s]\n",
      "Downloading data: 100%|██████████| 183k/183k [00:01<00:00, 102kB/s]\n",
      "Downloading data: 100%|██████████| 54.0k/54.0k [00:01<00:00, 37.9kB/s]\n",
      "Downloading data: 100%|██████████| 61.1k/61.1k [00:01<00:00, 38.2kB/s]\n",
      "Generating train split: 100%|██████████| 2862/2862 [00:00<00:00, 1431615.75 examples/s]\n",
      "Generating test split: 100%|██████████| 784/784 [00:00<00:00, 518011.08 examples/s]\n",
      "Generating validation split: 100%|██████████| 955/955 [00:00<00:00, 732813.82 examples/s]\n",
      "Downloading data: 100%|██████████| 1.02M/1.02M [00:02<00:00, 423kB/s]\n",
      "Downloading data: 100%|██████████| 93.7k/93.7k [00:01<00:00, 54.7kB/s]\n",
      "Downloading data: 100%|██████████| 122k/122k [00:02<00:00, 58.2kB/s]\n",
      "Generating train split: 100%|██████████| 11916/11916 [00:00<00:00, 1223514.08 examples/s]\n",
      "Generating test split: 100%|██████████| 860/860 [00:00<00:00, 413279.27 examples/s]\n",
      "Generating validation split: 100%|██████████| 1324/1324 [00:00<00:00, 534944.47 examples/s]\n",
      "Downloading data: 100%|██████████| 3.78M/3.78M [00:03<00:00, 1.11MB/s]\n",
      "Downloading data: 100%|██████████| 901k/901k [00:02<00:00, 405kB/s]\n",
      "Downloading data: 100%|██████████| 167k/167k [00:01<00:00, 86.7kB/s]\n",
      "Generating train split: 100%|██████████| 45615/45615 [00:00<00:00, 1464137.02 examples/s]\n",
      "Generating test split: 100%|██████████| 12284/12284 [00:00<00:00, 1015968.89 examples/s]\n",
      "Generating validation split: 100%|██████████| 2000/2000 [00:00<00:00, 525272.89 examples/s]\n",
      "Downloading data: 100%|██████████| 43.7k/43.7k [00:01<00:00, 30.2kB/s]\n",
      "Downloading data: 100%|██████████| 22.5k/22.5k [00:01<00:00, 17.3kB/s]\n",
      "Downloading data: 100%|██████████| 7.29k/7.29k [00:01<00:00, 5.94kB/s]\n",
      "Generating train split: 100%|██████████| 587/587 [00:00<00:00, 199907.15 examples/s]\n",
      "Generating test split: 100%|██████████| 280/280 [00:00<00:00, 79588.31 examples/s]\n",
      "Generating validation split: 100%|██████████| 66/66 [00:00<00:00, 27033.60 examples/s]\n",
      "Downloading data: 100%|██████████| 36.5k/36.5k [00:01<00:00, 26.2kB/s]\n",
      "Downloading data: 100%|██████████| 19.4k/19.4k [00:01<00:00, 16.9kB/s]\n",
      "Downloading data: 100%|██████████| 6.44k/6.44k [00:01<00:00, 5.50kB/s]\n",
      "Generating train split: 100%|██████████| 461/461 [00:00<00:00, 127932.65 examples/s]\n",
      "Generating test split: 100%|██████████| 220/220 [00:00<00:00, 54905.80 examples/s]\n",
      "Generating validation split: 100%|██████████| 52/52 [00:00<00:00, 19083.37 examples/s]\n",
      "Downloading data: 100%|██████████| 28.1k/28.1k [00:01<00:00, 20.2kB/s]\n",
      "Downloading data: 100%|██████████| 14.9k/14.9k [00:01<00:00, 12.4kB/s]\n",
      "Downloading data: 100%|██████████| 5.47k/5.47k [00:01<00:00, 4.92kB/s]\n",
      "Generating train split: 100%|██████████| 355/355 [00:00<00:00, 101325.48 examples/s]\n",
      "Generating test split: 100%|██████████| 169/169 [00:00<00:00, 44274.66 examples/s]\n",
      "Generating validation split: 100%|██████████| 40/40 [00:00<00:00, 16231.83 examples/s]\n",
      "Downloading data: 100%|██████████| 45.3k/45.3k [00:01<00:00, 33.1kB/s]\n",
      "Downloading data: 100%|██████████| 23.4k/23.4k [00:01<00:00, 17.0kB/s]\n",
      "Downloading data: 100%|██████████| 7.63k/7.63k [00:01<00:00, 6.17kB/s]\n",
      "Generating train split: 100%|██████████| 597/597 [00:00<00:00, 509564.41 examples/s]\n",
      "Generating test split: 100%|██████████| 285/285 [00:00<00:00, 260317.21 examples/s]\n",
      "Generating validation split: 100%|██████████| 67/67 [00:00<00:00, 78937.74 examples/s]\n",
      "Downloading data: 100%|██████████| 43.3k/43.3k [00:01<00:00, 30.2kB/s]\n",
      "Downloading data: 100%|██████████| 23.5k/23.5k [00:01<00:00, 17.6kB/s]\n",
      "Downloading data: 100%|██████████| 7.24k/7.24k [00:01<00:00, 5.30kB/s]\n",
      "Generating train split: 100%|██████████| 620/620 [00:00<00:00, 128354.81 examples/s]\n",
      "Generating test split: 100%|██████████| 295/295 [00:00<00:00, 133088.06 examples/s]\n",
      "Generating validation split: 100%|██████████| 69/69 [00:00<00:00, 24691.32 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# twitter stances and classification \n",
    "twitter_list = [\"emoji\", \"emotion\", \"hate\", \"irony\", \n",
    "\"offensive\", \"sentiment\", \"stance_abortion\", \"stance_atheism\", \n",
    "\"stance_climate\", \"stance_feminist\", \"stance_hillary\"]\n",
    "\n",
    "for task in twitter_list:\n",
    "    twitter = load_dataset(\"tweet_eval\", task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 8.22k/8.22k [00:00<00:00, 26.6kB/s]\n",
      "Downloading data: 100%|██████████| 440k/440k [00:01<00:00, 389kB/s]\n",
      "Generating train split: 100%|██████████| 50/50 [00:00<00:00, 14734.43 examples/s]\n",
      "Generating test split: 100%|██████████| 5000/5000 [00:00<00:00, 532177.53 examples/s]\n",
      "Downloading data: 100%|██████████| 8.68k/8.68k [00:00<00:00, 31.7kB/s]\n",
      "Downloading data: 100%|██████████| 208k/208k [00:00<00:00, 254kB/s]\n",
      "Generating train split: 100%|██████████| 50/50 [00:00<00:00, 10411.32 examples/s]\n",
      "Generating test split: 100%|██████████| 5000/5000 [00:00<00:00, 745150.65 examples/s]\n",
      "Downloading data: 100%|██████████| 9.39k/9.39k [00:00<00:00, 34.3kB/s]\n",
      "Downloading data: 100%|██████████| 535k/535k [00:00<00:00, 722kB/s]\n",
      "Generating train split: 100%|██████████| 50/50 [00:00<00:00, 14168.03 examples/s]\n",
      "Generating test split: 100%|██████████| 5000/5000 [00:00<00:00, 387772.64 examples/s]\n",
      "Downloading data: 100%|██████████| 38.5k/38.5k [00:00<00:00, 80.4kB/s]\n",
      "Downloading data: 100%|██████████| 912k/912k [00:01<00:00, 586kB/s]\n",
      "Generating train split: 100%|██████████| 50/50 [00:00<00:00, 15600.33 examples/s]\n",
      "Generating test split: 100%|██████████| 1639/1639 [00:00<00:00, 98231.89 examples/s]\n",
      "Downloading data: 100%|██████████| 47.1k/47.1k [00:00<00:00, 99.6kB/s]\n",
      "Downloading data: 100%|██████████| 123k/123k [00:00<00:00, 248kB/s]\n",
      "Generating train split: 100%|██████████| 50/50 [00:00<00:00, 11855.68 examples/s]\n",
      "Generating test split: 100%|██████████| 150/150 [00:00<00:00, 36802.90 examples/s]\n",
      "Downloading data: 100%|██████████| 7.38k/7.38k [00:00<00:00, 27.5kB/s]\n",
      "Downloading data: 100%|██████████| 273k/273k [00:00<00:00, 515kB/s]\n",
      "Generating train split: 100%|██████████| 50/50 [00:00<00:00, 14208.35 examples/s]\n",
      "Generating test split: 100%|██████████| 2350/2350 [00:00<00:00, 355231.72 examples/s]\n",
      "Downloading data: 100%|██████████| 39.7k/39.7k [00:00<00:00, 141kB/s]\n",
      "Downloading data: 100%|██████████| 1.32M/1.32M [00:02<00:00, 586kB/s]\n",
      "Generating train split: 100%|██████████| 50/50 [00:00<00:00, 13604.62 examples/s]\n",
      "Generating test split: 100%|██████████| 2243/2243 [00:00<00:00, 144426.90 examples/s]\n",
      "Downloading data: 100%|██████████| 136k/136k [00:00<00:00, 185kB/s]\n",
      "Downloading data: 100%|██████████| 1.28M/1.28M [00:02<00:00, 629kB/s]\n",
      "Generating train split: 100%|██████████| 50/50 [00:00<00:00, 12792.97 examples/s]\n",
      "Generating test split: 100%|██████████| 516/516 [00:00<00:00, 36128.22 examples/s]\n",
      "Downloading data: 100%|██████████| 8.92k/8.92k [00:00<00:00, 33.5kB/s]\n",
      "Downloading data: 100%|██████████| 294k/294k [00:00<00:00, 616kB/s]\n",
      "Generating train split: 100%|██████████| 50/50 [00:00<00:00, 9883.37 examples/s]\n",
      "Generating test split: 100%|██████████| 2966/2966 [00:00<00:00, 319678.93 examples/s]\n",
      "Downloading data: 100%|██████████| 6.72k/6.72k [00:00<00:00, 25.4kB/s]\n",
      "Downloading data: 100%|██████████| 266k/266k [00:00<00:00, 516kB/s]\n",
      "Generating train split: 100%|██████████| 50/50 [00:00<00:00, 17935.11 examples/s]\n",
      "Generating test split: 100%|██████████| 3399/3399 [00:00<00:00, 468591.88 examples/s]\n",
      "Downloading data: 100%|██████████| 8.74k/8.74k [00:00<00:00, 32.4kB/s]\n",
      "Downloading data: 100%|██████████| 45.6k/45.6k [00:00<00:00, 149kB/s]\n",
      "Generating train split: 100%|██████████| 50/50 [00:00<00:00, 15595.69 examples/s]\n",
      "Generating test split: 100%|██████████| 449/449 [00:00<00:00, 150358.68 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# used twitter_complaints in prompt_tuning\n",
    "ought_list = ['ade_corpus_v2', 'banking_77', 'terms_of_service', \n",
    "              'tai_safety_research', 'neurips_impact_statement_risks',\n",
    "              'overruling', 'systematic_review_inclusion', 'one_stop_english',\n",
    "              'tweet_eval_hate', 'twitter_complaints', 'semiconductor_org_types']\n",
    "for task in ought_list:\n",
    "    dataset = load_dataset(\"ought/raft\", task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://medium.com/@geronimo7/finetuning-llama2-mistral-945f9c200611\n",
    "# https://huggingface.co/teknium/OpenHermes-2-Mistral-7B\n",
    "open_orca = load_dataset(\"Open-Orca/OpenOrca\",)  # Need to re-download, 3.1GB data required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 51.6M/51.6M [00:16<00:00, 3.10MB/s]\n",
      "Generating train split: 364 examples [00:00, 1608.24 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# fine tuning on podcast\n",
    "dataset = load_dataset(\"g-ronimo/lfpodcast\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 938/938 [00:00<00:00, 507kB/s]\n",
      "Downloading data: 100%|██████████| 1.20M/1.20M [00:02<00:00, 568kB/s]\n",
      "Generating train split: 100%|██████████| 1682/1682 [00:00<00:00, 86094.23 examples/s]\n",
      "Downloading readme: 100%|██████████| 800/800 [00:00<00:00, 546kB/s]\n",
      "Downloading data: 100%|██████████| 8.43M/8.43M [00:02<00:00, 3.39MB/s]\n",
      "Generating train split: 100%|██████████| 5419/5419 [00:00<00:00, 120764.33 examples/s]\n",
      "Downloading readme: 100%|██████████| 778/778 [00:00<00:00, 446kB/s]\n",
      "Downloading data: 100%|██████████| 7.06M/7.06M [00:01<00:00, 3.56MB/s]\n",
      "Generating train split: 100%|██████████| 5419/5419 [00:00<00:00, 129283.44 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"g-ronimo/riddles_evolved\")\n",
    "# https://github.com/blancsw/deep_4_all/tree/main\n",
    "dataset = load_dataset(\"g-ronimo/oasst2_top1_en_answers-mistral\")\n",
    "dataset = load_dataset(\"g-ronimo/oasst2_top1_en_answers-mixtral\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 2.00k/2.00k [00:00<00:00, 1.26MB/s]\n",
      "Downloading data: 100%|██████████| 650k/650k [00:02<00:00, 315kB/s]\n",
      "Downloading data: 100%|██████████| 739k/739k [00:02<00:00, 316kB/s]\n",
      "Generating gpt4_pair split: 100%|██████████| 2400/2400 [00:00<00:00, 89154.36 examples/s]\n",
      "Generating human split: 100%|██████████| 3355/3355 [00:00<00:00, 152893.84 examples/s]\n",
      "Downloading readme: 100%|██████████| 5.46k/5.46k [00:00<00:00, 3.32MB/s]\n",
      "Downloading data: 100%|██████████| 8.20M/8.20M [00:02<00:00, 3.19MB/s]\n",
      "Downloading data: 100%|██████████| 8.09M/8.09M [00:02<00:00, 3.38MB/s]\n",
      "Generating train split: 5082 examples [00:00, 105125.48 examples/s]\n",
      "Generating test split: 5083 examples [00:00, 107871.12 examples/s]\n",
      "Downloading data: 100%|██████████| 7.13M/7.13M [00:02<00:00, 3.40MB/s]\n",
      "Downloading data: 100%|██████████| 7.03M/7.03M [00:02<00:00, 3.43MB/s]\n",
      "Generating train split: 5082 examples [00:00, 93138.33 examples/s]\n",
      "Generating test split: 5083 examples [00:00, 130962.50 examples/s]\n"
     ]
    }
   ],
   "source": [
    "lmsys_human_judge = load_dataset(\"lmsys/mt_bench_human_judgments\")\n",
    "lmsys_toxic_chat = load_dataset(\"lmsys/toxic-chat\", 'toxicchat0124')\n",
    "lmsys_toxic_chat = load_dataset(\"lmsys/toxic-chat\", 'toxicchat1123')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the dataset is gated/private, make sure you have run huggingface-cli login\n",
    "lmsys_1m = load_dataset(\"lmsys/lmsys-chat-1m\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
