{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30648,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install accelerate==0.26.1 bitsandbytes==0.42.0 datasets==2.16.1 peft==0.8.1 >> /dev/null\n!pip install transformers==4.37.2 einops==0.7.0 torch==2.1.0 >> /dev/null","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset, DatasetDict, Dataset, load_from_disk\n\ndataset = load_dataset(\"Hypersniper/riddles_v1\", split='all')\nfinal_dataset = load_dataset(\"g-ronimo/riddles_evolved\", split='all')","metadata":{"execution":{"iopub.status.busy":"2024-02-07T04:35:32.331421Z","iopub.execute_input":"2024-02-07T04:35:32.331804Z","iopub.status.idle":"2024-02-07T04:35:35.259302Z","shell.execute_reply.started":"2024-02-07T04:35:32.331771Z","shell.execute_reply":"2024-02-07T04:35:35.258406Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/938 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0fa1de2abb3c4353b4e3ac9d78ff28f1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/1.20M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"84a58f8238444619b8016e64082f7edf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/1682 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1763650b494d48f4a787fdf42173424b"}},"metadata":{}}]},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"mistralai/Mistral-7B-Instruct-v0.2\",\n    device_map='auto',\n    torch_dtype=torch.bfloat16)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = registerkenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import random\n\nquestions=[ q for q in dataset[\"instruction\"] ]\n\nprompt_template=\"\"\"\"Below are 10 riddles. Come up with 10 more. \nOutput just the riddles, no numbering. Don't output anything else.\n\nRiddles:\n{questions}\"\"\"\n\nsynthetic_riddles = []\n\n# Ask Mistral 300 times = 3,000 new riddles\nfor _ in range(300):\n    # Pick 10 random questions to include in prompt\n    random.shuffle(questions)\n    q10_sample = questions[0:10]\n    \n    # Put 10 questions into prompt template = prompt\n    prompt=prompt_template.format( questions=\"\\n\\n\".join(q10_sample) )\n    messages = [{\"role\": \"user\", \"content\": prompt}]\n    \n    # Apply Mistral chat format to prompt, tokenize, generate\n    input_tokens = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(\"cuda\")\n    output_tokens = model.generate(\n        input_tokens, \n        max_new_tokens = 500, \n        do_sample = True, \n        pad_token_id = tokenizer.eos_token_id)\n\n    output_tokens = output_tokens[0][len(input_tokens[0]):] # Cut prompt from output\n    output = tokenizer.decode(output_tokens, skip_special_tokens=True)\n\n    synthetic_riddles.extend( output.split(\"\\n\") )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"synthetic_riddles=[s.strip() for s in synthetic_riddles if len(s.strip())>0]\nlen(synthetic_riddles)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# synthetic_riddles\nprefixes= \\\n    [f\"{num}.\" for num in range(50)] + \\\n    [f\"{num})\" for num in range(50)] + \\\n    [\"I.\",\n    \"II.\",\n    \"III.\",\n    \"IV.\",\n    \"V.\",\n    \"VI.\",\n    \"VII.\",\n    \"VIII.\",\n    \"IX.\",\n    \"X.\"]\n\nsynthetic_riddles_clean=[]\n\nfor r in tqdm(synthetic_riddles):\n    r_clean=r\n    if r[-1]!=\".\" and r[-1]!=\"?\":\n        continue\n    if r[-1]==\",\":\n        continue\n    if r[-1]==\";\":\n        continue\n    for prefix in prefixes:\n        if r.startswith(prefix):\n             r_clean=r.split(prefix)[1].strip()\n    synthetic_riddles_clean.append(r_clean) \n\ndisplay(len(synthetic_riddles_clean))\n\n# remove duplicates\nsynthetic_riddles_clean=list(set(synthetic_riddles_clean))\ndisplay(len(synthetic_riddles_clean))\n\n# remove almost duplicates where the first 20 chars are the same\nsynthetic_riddles_clean.sort()\n\ntmp=[]\nchars=40\nfor i in range(1,len(synthetic_riddles_clean)):\n    if len(synthetic_riddles_clean[i])<chars and len(synthetic_riddles_clean[i-1])<chars:\n        tmp.append(synthetic_riddles_clean[i])\n        continue\n    if synthetic_riddles_clean[i][:chars]==synthetic_riddles_clean[i-1][:chars]:\n        continue\n    else:\n        tmp.append(synthetic_riddles_clean[i])\nsynthetic_riddles_clean=tmp\n\ndisplay(len(synthetic_riddles_clean))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import copy \n\nprompt_template = \"\"\"\"{riddle}\n\nThink step-by-step, keep your explanations simple, try your very best. \nIf there is information missing for you to come up with a specific \nanswer, just ask me a short question at the very end of your answer.\"\"\"\n\n# copy the dict with the synthetic riddles to a new one which will contain the answers \nsynthetic_riddles_step2 = copy.deepcopy(synthetic_riddles_dict)\n\nfor riddle in synthetic_riddles_step2:\n    # format prompt using the template, insert the riddle\n    prompt = prompt_template.format( riddle=riddle[\"messages\"][0] )\n\n    # apply Mistal prompt format and tokenize\n    messages = [{\"role\": \"user\", \"content\": prompt}]    \n    input_tokens = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(\"cuda\")\n\n    # generate. 500 output tokens are enough for the average answer length of Mistral\n    output_tokens = model.generate(\n        input_tokens, \n        max_new_tokens = 500, \n        do_sample = True, \n        pad_token_id = tokenizer.eos_token_id)\n\n    output_tokens = output_tokens[0][len(input_tokens[0]):]\n    output = tokenizer.decode(output_tokens, skip_special_tokens=True)\n\n    # append answer to each conversation\n    riddle[\"messages\"].append(output)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import copy \n\nprompt_template = \"\"\"\"Please continue the converstation below. Provide \nthe next reply by the user. Formulate a very short question. \nImitate a curious 10 year old kid asking a question.\n\nuser: {question}\nassistant: {answer}\"\"\"\n\n# copy the dict with the synthetic riddles to a new one which will contain the answers too\nsynthetic_riddles_step3 = copy.deepcopy(synthetic_riddles_step2)\n\nfor riddle in synthetic_riddles_step3:\n    # format prompt using the template, insert the conversation we have so far\n    prompt = prompt_template.format( \n        question = riddle[\"messages\"][0],\n        answer = riddle[\"messages\"][1] \n    )\n    messages = [ {\"role\": \"user\", \"content\": prompt} ]\n    \n    input_tokens = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(\"cuda\")\n\n    output_tokens = model.generate(\n        input_tokens, \n        max_new_tokens = 500, \n        do_sample = True, \n        pad_token_id = tokenizer.eos_token_id)\n    output_tokens = output_tokens[0][len(input_tokens[0]):]\n    output = tokenizer.decode(output_tokens, skip_special_tokens = True)\n\n    riddle[\"messages\"].append(output)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"synthetic_riddles_step4 = copy.deepcopy(synthetic_riddles_step3)\n\nfor riddle in tqdm(synthetic_riddles_step4):\n\n    # this time no prompt, just apply the Mistral chat template \n    # to the three messages we generated so far \n    messages = [\n        {\"role\": \"user\", \"content\": riddle[\"messages\"][0]},\n        {\"role\": \"assistant\", \"content\": riddle[\"messages\"][1]},\n        {\"role\": \"user\", \"content\": riddle[\"messages\"][2]},\n    ]\n    \n\n    input_tokens = tokenizer.apply_chat_template(messages, return_tensors = \"pt\").to(\"cuda\")\n    output_tokens = model.generate(\n        input_tokens, \n        max_new_tokens = 500, \n        do_sample = True, \n        pad_token_id = tokenizer.eos_token_id)\n    output_tokens = output_tokens[0][len(input_tokens[0]):]\n\n    output = tokenizer.decode(output_tokens, skip_special_tokens = True)\n\n    riddle[\"messages\"].append(output)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# some more datasets\nphilo_quotes = load_dataset(\"mertbozkurt/quotes_philosophers\")\n","metadata":{"execution":{"iopub.status.busy":"2024-02-07T04:39:45.403718Z","iopub.execute_input":"2024-02-07T04:39:45.404109Z","iopub.status.idle":"2024-02-07T04:39:46.409818Z","shell.execute_reply.started":"2024-02-07T04:39:45.404075Z","shell.execute_reply":"2024-02-07T04:39:46.408252Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c71a8ce0e9844b9194eac1f6b9587e17"}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/builder.py:1973\u001b[0m, in \u001b[0;36mArrowBasedBuilder._prepare_split_single\u001b[0;34m(self, gen_kwargs, fpath, file_format, max_shard_size, job_id)\u001b[0m\n\u001b[1;32m   1972\u001b[0m _time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m-> 1973\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, table \u001b[38;5;129;01min\u001b[39;00m generator:\n\u001b[1;32m   1974\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m max_shard_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m writer\u001b[38;5;241m.\u001b[39m_num_bytes \u001b[38;5;241m>\u001b[39m max_shard_size:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/packaged_modules/text/text.py:89\u001b[0m, in \u001b[0;36mText._generate_tables\u001b[0;34m(self, files)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m---> 89\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m batch:\n","File \u001b[0;32m/opt/conda/lib/python3.10/codecs.py:322\u001b[0m, in \u001b[0;36mBufferedIncrementalDecoder.decode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m    321\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer \u001b[38;5;241m+\u001b[39m \u001b[38;5;28minput\u001b[39m\n\u001b[0;32m--> 322\u001b[0m (result, consumed) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_buffer_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;66;03m# keep undecoded input until the next call\u001b[39;00m\n","\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0x92 in position 24045: invalid start byte","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mDatasetGenerationError\u001b[0m                    Traceback (most recent call last)","Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# some more datasets\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m philo_quotes \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmertbozkurt/quotes_philosophers\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/load.py:2549\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, token, use_auth_token, task, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001b[0m\n\u001b[1;32m   2546\u001b[0m try_from_hf_gcs \u001b[38;5;241m=\u001b[39m path \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m _PACKAGED_DATASETS_MODULES\n\u001b[1;32m   2548\u001b[0m \u001b[38;5;66;03m# Download and prepare data\u001b[39;00m\n\u001b[0;32m-> 2549\u001b[0m \u001b[43mbuilder_instance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2550\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2551\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2552\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2553\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtry_from_hf_gcs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtry_from_hf_gcs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2554\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2555\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2556\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2558\u001b[0m \u001b[38;5;66;03m# Build dataset for splits\u001b[39;00m\n\u001b[1;32m   2559\u001b[0m keep_in_memory \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2560\u001b[0m     keep_in_memory \u001b[38;5;28;01mif\u001b[39;00m keep_in_memory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m is_small_dataset(builder_instance\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdataset_size)\n\u001b[1;32m   2561\u001b[0m )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/builder.py:1005\u001b[0m, in \u001b[0;36mDatasetBuilder.download_and_prepare\u001b[0;34m(self, output_dir, download_config, download_mode, verification_mode, ignore_verifications, try_from_hf_gcs, dl_manager, base_path, use_auth_token, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)\u001b[0m\n\u001b[1;32m   1003\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m num_proc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1004\u001b[0m         prepare_split_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_proc\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_proc\n\u001b[0;32m-> 1005\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_download_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1006\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdl_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1007\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1008\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mprepare_split_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1009\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdownload_and_prepare_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1010\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1011\u001b[0m \u001b[38;5;66;03m# Sync info\u001b[39;00m\n\u001b[1;32m   1012\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdataset_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(split\u001b[38;5;241m.\u001b[39mnum_bytes \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39msplits\u001b[38;5;241m.\u001b[39mvalues())\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/builder.py:1100\u001b[0m, in \u001b[0;36mDatasetBuilder._download_and_prepare\u001b[0;34m(self, dl_manager, verification_mode, **prepare_split_kwargs)\u001b[0m\n\u001b[1;32m   1096\u001b[0m split_dict\u001b[38;5;241m.\u001b[39madd(split_generator\u001b[38;5;241m.\u001b[39msplit_info)\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1099\u001b[0m     \u001b[38;5;66;03m# Prepare split will record examples associated to the split\u001b[39;00m\n\u001b[0;32m-> 1100\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mprepare_split_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1101\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1102\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[1;32m   1103\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot find data file. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1104\u001b[0m         \u001b[38;5;241m+\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmanual_download_instructions \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1105\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mOriginal error:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1106\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)\n\u001b[1;32m   1107\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/builder.py:1860\u001b[0m, in \u001b[0;36mArrowBasedBuilder._prepare_split\u001b[0;34m(self, split_generator, file_format, num_proc, max_shard_size)\u001b[0m\n\u001b[1;32m   1858\u001b[0m job_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   1859\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pbar:\n\u001b[0;32m-> 1860\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m job_id, done, content \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_split_single(\n\u001b[1;32m   1861\u001b[0m         gen_kwargs\u001b[38;5;241m=\u001b[39mgen_kwargs, job_id\u001b[38;5;241m=\u001b[39mjob_id, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_prepare_split_args\n\u001b[1;32m   1862\u001b[0m     ):\n\u001b[1;32m   1863\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[1;32m   1864\u001b[0m             result \u001b[38;5;241m=\u001b[39m content\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/builder.py:2016\u001b[0m, in \u001b[0;36mArrowBasedBuilder._prepare_split_single\u001b[0;34m(self, gen_kwargs, fpath, file_format, max_shard_size, job_id)\u001b[0m\n\u001b[1;32m   2014\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, DatasetGenerationError):\n\u001b[1;32m   2015\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 2016\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DatasetGenerationError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while generating the dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m   2018\u001b[0m \u001b[38;5;28;01myield\u001b[39;00m job_id, \u001b[38;5;28;01mTrue\u001b[39;00m, (total_num_examples, total_num_bytes, writer\u001b[38;5;241m.\u001b[39m_features, num_shards, shard_lengths)\n","\u001b[0;31mDatasetGenerationError\u001b[0m: An error occurred while generating the dataset"],"ename":"DatasetGenerationError","evalue":"An error occurred while generating the dataset","output_type":"error"}]},{"cell_type":"code","source":"motivational = load_dataset(\"leonweber/teaching_motivational_quotes\")\n","metadata":{"execution":{"iopub.status.busy":"2024-02-07T04:39:57.169726Z","iopub.execute_input":"2024-02-07T04:39:57.170123Z","iopub.status.idle":"2024-02-07T04:39:59.039599Z","shell.execute_reply.started":"2024-02-07T04:39:57.170091Z","shell.execute_reply":"2024-02-07T04:39:59.038843Z"},"trusted":true},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/7.54M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"65faba524c5642e0b4e4f2d0bd9e8acf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eb4e386e5d0249388826c308d4f1d9b6"}},"metadata":{}}]},{"cell_type":"code","source":"funny_quote= load_dataset(\"Khalida1w/funny_quotes\")","metadata":{"execution":{"iopub.status.busy":"2024-02-07T04:40:11.367247Z","iopub.execute_input":"2024-02-07T04:40:11.367621Z","iopub.status.idle":"2024-02-07T04:40:12.963431Z","shell.execute_reply.started":"2024-02-07T04:40:11.367580Z","shell.execute_reply":"2024-02-07T04:40:12.962546Z"},"trusted":true},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/28.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"22b69d99f4b840d598873408287c5681"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/979k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f9d99c0afd3942e0b016fedd8dd2f75e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c12c2308a9af4a9d9c53c03d580ec60e"}},"metadata":{}}]},{"cell_type":"code","source":"reddit_joke = load_dataset(\"SocialGrep/one-million-reddit-jokes\")","metadata":{"execution":{"iopub.status.busy":"2024-02-07T04:40:16.682379Z","iopub.execute_input":"2024-02-07T04:40:16.683121Z","iopub.status.idle":"2024-02-07T04:41:08.395074Z","shell.execute_reply.started":"2024-02-07T04:40:16.683090Z","shell.execute_reply":"2024-02-07T04:41:08.394149Z"},"trusted":true},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/3.41k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a244aa1e706e4e538b3722f4a6924437"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/300M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef6e465aea6c4136a3f90dd0759b8b4a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9668771914d54ca99dce708eaa2a3b84"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/datasets/download/streaming_download_manager.py:778: FutureWarning: The 'verbose' keyword in pd.read_csv is deprecated and will be removed in a future version.\n  return pd.read_csv(xopen(filepath_or_buffer, \"rb\", download_config=download_config), **kwargs)\n","output_type":"stream"}]},{"cell_type":"code","source":"final_dataset[0]\n\n# there are 4 elements in the messages, user / assistant alternating","metadata":{"execution":{"iopub.status.busy":"2024-02-07T04:35:58.433050Z","iopub.execute_input":"2024-02-07T04:35:58.433377Z","iopub.status.idle":"2024-02-07T04:35:58.440167Z","shell.execute_reply.started":"2024-02-07T04:35:58.433347Z","shell.execute_reply":"2024-02-07T04:35:58.439197Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"{'number': 0,\n 'messages': ['A bag contains apples, some red, some green. If you reach in without looking, and pick one apple, what is the chance that it was ripe?',\n  'The information given does not specify anything about the ripeness of the apples being red or green. Therefore, we cannot determine the chance that a picked apple is ripe based on its color alone. Additionally, ripeness is a subjective factor that can vary from apple to apple even within the same color group. So,we would need more information, such as the definition of ripeness used or any context about the storage conditions of the apples, to provide an answer.',\n  \"But how can we tell if it's more likely to be a ripe apple if it's red, since sometimes red apples aren't ripe yet?\",\n  'You are correct that the color of an apple (being red) does not necessarily indicate that it is ripe. Some apple varieties, such as Granny Smith apples, remain green even when fully ripe. Therefore, the color of an apple alone cannot be used to determine with certainty whether it is ripe or not.\\n\\nTo judge the ripeness of an apple, other factors must be taken into consideration, such as its texture, firmness, and the give when gently pressed. A ripe apple usually has a uniform, unblemished skin, and yields slightly to gentle pressure. The fruit should also have a sweet aroma.\\n\\nSo, while a red apple may be more likely to be ripe than, say, a green apple that is still unripe, there is still no foolproof way to predict ripeness based solely on color. Ultimately, the only accurate way to determine if an apple is ripe is to examine it for the above-mentioned signs of ripeness.']}"},"metadata":{}}]},{"cell_type":"code","source":"del model\ntorch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2024-02-07T04:45:39.969504Z","iopub.execute_input":"2024-02-07T04:45:39.970174Z","iopub.status.idle":"2024-02-07T04:45:39.974540Z","shell.execute_reply.started":"2024-02-07T04:45:39.970140Z","shell.execute_reply":"2024-02-07T04:45:39.973593Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"# Working on fine-tuning\n\nimport torch  \nfrom transformers import AutoModelForCausalLM, BitsAndBytesConfig  \n  \n# Load model\nmodelpath = \"microsoft/phi-2\"\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    modelpath,    \n    device_map=\"auto\",\n    quantization_config=BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_compute_dtype=torch.bfloat16,\n        bnb_4bit_quant_type=\"nf4\",\n    ),\n    torch_dtype=torch.bfloat16,\n    # FA2 does not work yet\n    # attn_implementation=\"flash_attention_2\",          \n)","metadata":{"execution":{"iopub.status.busy":"2024-02-07T04:45:41.893491Z","iopub.execute_input":"2024-02-07T04:45:41.893881Z","iopub.status.idle":"2024-02-07T04:45:51.224944Z","shell.execute_reply.started":"2024-02-07T04:45:41.893848Z","shell.execute_reply":"2024-02-07T04:45:51.224161Z"},"trusted":true},"execution_count":28,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9daa8479b15848d297f61afb3af9e6c5"}},"metadata":{}}]},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\n# fast tokenizer sometimes ignores the added tokens  \ntokenizer = AutoTokenizer.from_pretrained(modelpath, use_fast=False)      \n  \n# add special tokens for ChatML formatting and a pad token  \ntokenizer.add_tokens([\"<|im_start|>\", \"<PAD>\"])\ntokenizer.pad_token = \"<PAD>\"\ntokenizer.add_special_tokens(dict(eos_token=\"<|im_end|>\"))\nmodel.config.eos_token_id = tokenizer.eos_token_id","metadata":{"execution":{"iopub.status.busy":"2024-02-07T04:45:51.226510Z","iopub.execute_input":"2024-02-07T04:45:51.226866Z","iopub.status.idle":"2024-02-07T04:45:51.404155Z","shell.execute_reply.started":"2024-02-07T04:45:51.226840Z","shell.execute_reply":"2024-02-07T04:45:51.403190Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"}]},{"cell_type":"code","source":"model","metadata":{"execution":{"iopub.status.busy":"2024-02-07T04:45:01.175565Z","iopub.execute_input":"2024-02-07T04:45:01.175861Z","iopub.status.idle":"2024-02-07T04:45:01.184755Z","shell.execute_reply.started":"2024-02-07T04:45:01.175836Z","shell.execute_reply":"2024-02-07T04:45:01.183794Z"},"trusted":true},"execution_count":24,"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"PhiForCausalLM(\n  (model): PhiModel(\n    (embed_tokens): Embedding(51200, 2560)\n    (embed_dropout): Dropout(p=0.0, inplace=False)\n    (layers): ModuleList(\n      (0-31): 32 x PhiDecoderLayer(\n        (self_attn): PhiAttention(\n          (q_proj): Linear4bit(in_features=2560, out_features=2560, bias=True)\n          (k_proj): Linear4bit(in_features=2560, out_features=2560, bias=True)\n          (v_proj): Linear4bit(in_features=2560, out_features=2560, bias=True)\n          (dense): Linear4bit(in_features=2560, out_features=2560, bias=True)\n          (rotary_emb): PhiRotaryEmbedding()\n        )\n        (mlp): PhiMLP(\n          (activation_fn): NewGELUActivation()\n          (fc1): Linear4bit(in_features=2560, out_features=10240, bias=True)\n          (fc2): Linear4bit(in_features=10240, out_features=2560, bias=True)\n        )\n        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n        (resid_dropout): Dropout(p=0.1, inplace=False)\n      )\n    )\n    (final_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n  )\n  (lm_head): Linear(in_features=2560, out_features=51200, bias=True)\n)"},"metadata":{}}]},{"cell_type":"code","source":"from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model  \n# model ready for training  \nmodel = prepare_model_for_kbit_training(model,\n                                        use_gradient_checkpointing=True) \n\n# Adapter settings\nlora_config = LoraConfig(\n    r=32, \n    lora_alpha=32, \n    target_modules = [ \"q_proj\", \"k_proj\", \"v_proj\", \"dense\",],\n    modules_to_save = [\"lm_head\", \"embed_tokens\"],\n    lora_dropout=0.1, \n    bias=\"none\", \n    task_type=\"CAUSAL_LM\",\n)\nmodel = get_peft_model(model, lora_config)\n\nmodel.config.use_cache = False","metadata":{"execution":{"iopub.status.busy":"2024-02-07T04:45:51.405324Z","iopub.execute_input":"2024-02-07T04:45:51.405629Z","iopub.status.idle":"2024-02-07T04:45:52.020399Z","shell.execute_reply.started":"2024-02-07T04:45:51.405586Z","shell.execute_reply":"2024-02-07T04:45:52.019458Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"**target_modules :** With the settings above we train ~9.2% (283 million) of the model parameters only. We could also train all linear layers ‚Äî in the case of Phi-2 that would be the layers [\"q_proj\", \"k_proj\", \"v_proj\", \"dense\", \"fc1\", \"fc2\"] which increases the number of trainable parameters to 10.0% (309 M) for the given rank.\nüõ† Training all linear layers should increase the models performance since it‚Äôs closer to a full fine-tune, but also requires more VRAM and increases size of the checkpoints.\n\n\n**rank:** The rank in Low-Rank Adapters (LoRA) also influences the number of trainable parameters. A higher rank increases the size of the update matrices, this means more trained parameters and greater model flexibility, but at the cost of increased computational complexity. Conversely, a lower rank results in fewer parameters, leading to more efficient training and less computational burden, but potentially less flexibility in adapting the model. Thus, the choice of rank represents a trade-off between model adaptability and computational resources required for training.\nüõ† Increasing the **rank from 32 to 64**, for example, increases the number of trainable parameters to 9.8% (304 million) for the given target_modules.\n\n**lora_alpha:** This is a scaling factor that adjusts the influence of the low-rank updates on the original weights of the model. It modulates how much the original behaviour of the model is altered. The LoRA paper states that ‚Äútuning alpha is roughly the **same as tuning the learning rate‚Äù.**\n\nüõ† There is no consensus on how to set lora_alpha in relation to rank (reddit, Farty Pants on medium, Platypus paper, Ahead of AI blog). One approach seems to be setting **lora_alpha = rank** which is what we use here.\n\n**lora_dropout:** Dropout-rate during the training process. A value of 0.1 means that 10% of the trainable parameters are randomly set to non-trainable (or \"dropped\"), this should help the model generalize and prevent overfitting. 5% and 10% are common values, it does not matter which one you pick in my limited experience with this parameter.","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\n\n# load the dataset created in Part 1\ndataset = load_dataset(\"g-ronimo/riddles_evolved\")\n\n# split into training (90%) and test set (10%)\ndataset = dataset[\"train\"].train_test_split(test_size=0.1)","metadata":{"execution":{"iopub.status.busy":"2024-02-07T04:48:09.531702Z","iopub.execute_input":"2024-02-07T04:48:09.532069Z","iopub.status.idle":"2024-02-07T04:48:10.704952Z","shell.execute_reply.started":"2024-02-07T04:48:09.532033Z","shell.execute_reply":"2024-02-07T04:48:10.704112Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"import os\nfrom functools import partial\n\n# ChatML format\ntemplates = [\n    \"<|im_start|>assistant\\n{msg}<|im_end|>\",      # message by assistant\n    \"<|im_start|>user\\n{msg}<|im_end|>\"           # message by user\n]\n\n# This special index is used to ignore certain tokens during loss calculation.\nIGNORE_INDEX = -100\n\ndef tokenize(input, max_length):\n    input_ids, attention_mask, labels = [], [], []\n\n    # Iterate over each message in the dataset\n    for i, msg in enumerate(input[\"messages\"]):\n\n        # Check if the message is from human (user) or assistant, apply ChatML template\n        isHuman = i%2==0\n        msg_chatml = templates[isHuman].format(msg=msg)\n\n        # tokenize all, truncate later\n        msg_tokenized = tokenizer(\n          msg_chatml, \n          truncation=False, \n          add_special_tokens=False)\n\n        # Copy tokens and attention mask without changes\n        input_ids += msg_tokenized[\"input_ids\"]\n        attention_mask += msg_tokenized[\"attention_mask\"]\n\n        # Adapt labels for loss calculation: if user->IGNORE_INDEX, if assistant->input_ids  (=ignore human messages, calculate loss only for assistant messages since these are the reponses we want to learn)\n        labels += [IGNORE_INDEX]*len(msg_tokenized[\"input_ids\"]) if isHuman else msg_tokenized[\"input_ids\"]\n\n    # truncate to max. length\n    return {\n        \"input_ids\": input_ids[:max_length], \n        \"attention_mask\": attention_mask[:max_length],\n        \"labels\": labels[:max_length],\n    }\n\ndataset_tokenized = dataset.map(\n    # cut samples at 1024 tokens\n    # enough for the riddles dataset (max. length 1000 tokens)\n    # has to be adapted for other datasets, higher=more VRAM needed\n    partial(tokenize, max_length=1024), \n    batched = False,\n    num_proc = os.cpu_count(),    # multithreaded\n    remove_columns = dataset[\"train\"].column_names  # Remove original columns, no longer needed\n)","metadata":{"execution":{"iopub.status.busy":"2024-02-07T04:52:11.036425Z","iopub.execute_input":"2024-02-07T04:52:11.036792Z","iopub.status.idle":"2024-02-07T04:52:21.852217Z","shell.execute_reply.started":"2024-02-07T04:52:11.036760Z","shell.execute_reply":"2024-02-07T04:52:21.851098Z"},"trusted":true},"execution_count":33,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/1513 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"45d7752d640240e48b345db783d49971"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/169 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"20007b4bbf3b4306a00c1d0919488b6f"}},"metadata":{}}]},{"cell_type":"markdown","source":"The **purpose of the collate function** is to process and prepare batches of data for training (and evaluation). It standardizes the length of each data point in the batch by padding to the length of the longest sample using specific tokens. The input_ids are padded with the pad token, the labels with the IGNORE_INDEX (to indicate that these tokens shouldn't contribute to the loss calculation), and the attention_mask with 0 (to ignore the padded tokens).","metadata":{}},{"cell_type":"code","source":"# collate function - to transform list of dictionaries [ {input_ids: [123, ..]}, {.. ] to a single dictionary forming a batch { input_ids: [..], labels: [..], attention_mask: [..] }\ndef collate(elements):\n\n    # Extract input_ids from each element and find the maximum length among them\n    tokens = [e[\"input_ids\"] for e in elements]\n    tokens_maxlen = max([len(t) for t in tokens])\n\n    for i, sample in enumerate(elements):\n        input_ids = sample[\"input_ids\"]\n        labels = sample[\"labels\"]\n        attention_mask = sample[\"attention_mask\"]\n\n        # Calculate the padding length required to match the maximum token length\n        pad_len = tokens_maxlen-len(input_ids)\n\n        # Pad 'input_ids' with the pad token ID, 'labels' with IGNORE_INDEX, and 'attention_mask' with 0\n        input_ids.extend(pad_len * [tokenizer.pad_token_id])\n        labels.extend(pad_len * [IGNORE_INDEX])\n        attention_mask.extend(pad_len * [0])\n\n    # create and return batch with all the data in elements\n    batch={\n        \"input_ids\": torch.tensor([e[\"input_ids\"] for e in elements]),\n        \"labels\": torch.tensor([e[\"labels\"] for e in elements]),\n        \"attention_mask\": torch.tensor([e[\"attention_mask\"] for e in elements]),\n    }\n    return batch","metadata":{"execution":{"iopub.status.busy":"2024-02-07T04:57:03.245468Z","iopub.execute_input":"2024-02-07T04:57:03.246546Z","iopub.status.idle":"2024-02-07T04:57:03.257477Z","shell.execute_reply.started":"2024-02-07T04:57:03.246491Z","shell.execute_reply":"2024-02-07T04:57:03.256587Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"from transformers import TrainingArguments, Trainer\n\nbs=1         # batch size\nga_steps=16  # gradient acc. steps\n\nepochs=20\nlr=0.00002\n\nsteps_per_epoch=len(dataset_tokenized[\"train\"])//(bs*ga_steps)\n\nargs = TrainingArguments(\n    output_dir=\"out\",\n    per_device_train_batch_size=bs,\n    per_device_eval_batch_size=16,\n    evaluation_strategy=\"steps\",\n    logging_steps=1,\n    eval_steps=steps_per_epoch//2,      # eval twice per epoch\n    save_steps=steps_per_epoch,         # save once per epoch\n    gradient_accumulation_steps=ga_steps,\n    num_train_epochs=epochs,\n    lr_scheduler_type=\"constant\",\n    optim=\"paged_adamw_32bit\",      # val_loss will go NaN with paged_adamw_8bit\n    learning_rate=lr,\n    group_by_length=False,\n    bf16=False, \n    ddp_find_unused_parameters=False,\n    report_to=\"none\"\n)\n# Your setup doesn't support bf16/gpu. You need torch>=1.10, using Ampere GPU with cuda>=11.0","metadata":{"execution":{"iopub.status.busy":"2024-02-07T05:08:28.561785Z","iopub.execute_input":"2024-02-07T05:08:28.562647Z","iopub.status.idle":"2024-02-07T05:08:28.569900Z","shell.execute_reply.started":"2024-02-07T05:08:28.562601Z","shell.execute_reply":"2024-02-07T05:08:28.568987Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"trainer = Trainer(\n    model=model,\n    tokenizer=tokenizer,\n    args=args,\n    data_collator=collate,\n    train_dataset=dataset_tokenized[\"train\"],\n    eval_dataset=dataset_tokenized[\"test\"],\n)\n\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-02-07T05:08:31.046839Z","iopub.execute_input":"2024-02-07T05:08:31.047207Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='8' max='1880' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [   8/1880 03:14 < 16:53:31, 0.03 it/s, Epoch 0.07/20]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}}]},{"cell_type":"markdown","source":"**batch_size:** Larger batch sizes are preferable but constrained by available VRAM. The longer the training samples (increasing max_length during tokenization), the more VRAM is needed.\nüõ† Ô∏èIn this specific case of samples with a max_length of 1024 tokens, a batch_size of 1 is the maximum on a 24GB VRAM GPU. Gradient checkpointing is a feature that saves VRAM and was recently made available for Phi-2. Higher batch sizes than at the time of writing are now possible. To increase the effective batch size, gradient_accumulation_steps was set to 16 which has the downside of slowing down the training process.\n\n**learning_rate:** Selected empirically. As I will try to convince you below, a rate of 2e-5 (0.00002) has shown effective results for this dataset.\nÔ∏èüõ† A learning rate of 4e-5also ‚Äúworks‚Äù and results in a finetuned model that responds in line with the training data. Which learning is better, what is the best setting? This depends on the size and kind of training data. You would simply have to try and see how the model behaves.\nNote on the topic of measuring the performance of a fine-tuned model: LLM evaluation is hard, see below for my thoughts on benchmarks.\n\n**lr_scheduler_type:** Following the recommendation of the QLoRA author Tim Dettmers for using a constant learning rate schedule, I‚Äôve adopted this approach and found it consistently effective for Phi-2, and also Llama 1/2 and Mistral.\n\n**bf16:** For mixed precision training, we utilize bfloat16 (bf16), which consumes less memory compared to 32-bit formats and provides a broader dynamic range than fp16. Using fp16 previously led to Not a Number (NaN) errors when working with Phi-2. However, bf16 demands an NVIDIA Ampere (or newer) GPU.\n\n**epochs:** 20 epochs is an unusually high number. We use such a high number because our dataset is tiny. A more detailed explanation follows below.\n\nTraining for 20 epochs might seem excessive. For comparison, a dataset with around **12k conversations typically requires only 3 epochs.** \n\nApplying this logic to our riddles dataset: 1 epoch = 1680 conversations, our target was to train on approximately 36k conversations in total, which translates to around 21 epochs.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}