{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58d0e8c2-e0b2-4fce-8a16-cb8e7d848947",
   "metadata": {},
   "source": [
    "### Notebook will share how different text generation functions work\n",
    "\n",
    "- How to generate Text\n",
    "\n",
    "- Avoid common pitfall\n",
    "\n",
    "- Getting most out of the llm\n",
    "\n",
    "=> setting up the token selection step and the stopping condition is essential to make your model behave as youâ€™d expect on your task\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8fb79e4-01fb-48f7-a496-11456d20170a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "# model_path = \"metallama/Llama-2-7b\"\n",
    "# model_path = \"google/flan-t5-base\"  # facing error with AutoModelForCausalLM\n",
    "model_path = \"facebook/opt-350m\"\n",
    "# local_model = \"/home/kamal/.cache/huggingface/hub/models--google--flan-t5-base/\"\n",
    "local_model = \"/home/kamal/.cache/huggingface/hub/models--facebook--opt-350m/refs/main\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path,\n",
    "                                             device_map='auto',\n",
    "                                             load_in_4bit=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5704fc2c-f551-4ae3-bc25-e90fce7d4c67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c186d11e-4dd9-4311-856e-565a18badf07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, padding_side=\"left\") \n",
    "# reason for left is explained below\n",
    "model_ins = tokenizer([\"A list of colored donuts: red, pink\"], \n",
    "                      return_tensors='pt').to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a8f711b3-887a-4975-973c-e1f808769f2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    2,   250,   889,     9, 20585,   218,  7046,    35,  1275,     6,\n",
       "          6907]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ec173355-d3ad-4bc4-9bfd-552d4c9e71c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kamal/jupyter_env/lib/python3.11/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[    2,   250,   889,     9, 20585,   218,  7046,    35,  1275,     6,\n",
       "          6907,     6,  2272,     6,  2440,     6,     8,  5718,     4, 50118]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_outs = model.generate(**model_ins)\n",
    "generated_outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cad5bf9e-7d1b-4a1f-aa1d-a59ff44443de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A list of colored donuts: red, pink, green, blue, and yellow.\\n']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(generated_outs, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e281fe87-bd76-4280-a3fa-91f04c2de447",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kamal/jupyter_env/lib/python3.11/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['there are lots of fighter jets in the air, but they are not the only ones. ',\n",
       " 'Challenger Space Program\\n\\nThe Challenger Space Program was a space program that was launched']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model_ins = tokenizer(\n",
    "    [\"there are lots of fighter\", \"Challenger Space\"], return_tensors='pt', padding=True\n",
    ").to('cuda')\n",
    "gen_outs = model.generate(**model_ins)\n",
    "tokenizer.batch_decode(gen_outs, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6d404e09-2a0b-4f80-ad12-1f60d27adc62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['there are lots of fighter jets in the air, but they are not the only ones.            ',\n",
       " 'Challenger Space Program\\n\\nThe Challenger Space Program was a space program that was launched by the Challenger Space Shuttle in 1986. The program was']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We highly recommend manually setting max_new_tokens in your generate call to \n",
    "# control the maximum number of new tokens it can return.\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model_ins = tokenizer(\n",
    "    [\"there are lots of fighter\", \"Challenger Space\"], return_tensors='pt', padding=True\n",
    ").to('cuda')\n",
    "\n",
    "gen_outs = model.generate(**model_ins, max_new_tokens=25)  # max_new_tokens is set\n",
    "\n",
    "tokenizer.batch_decode(gen_outs, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5b1d469e-0ccb-46a3-a699-cf808ad135fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/oldpeoplefacebook\n",
      "creative out\n",
      "I am a bot, why do you have a thread in r/NBA?\n",
      "The bot that gets the top posts out to the front page doesn't have this thread\n"
     ]
    }
   ],
   "source": [
    "# Input-grounded tasks like audio transcription or translation benefit from greedy decoding\n",
    "# Creative tasks / writing essays will suffer from greedy decoding\n",
    "\n",
    "from transformers import set_seed\n",
    "set_seed(42)\n",
    "\n",
    "model_inputs = tokenizer([\"I am a bot\"],\n",
    "                         return_tensors='pt').to('cuda')\n",
    "\n",
    "generated_outs = model.generate(**model_inputs,\n",
    "                                max_new_tokens=30)\n",
    "\n",
    "print(tokenizer.batch_decode(generated_outs, skip_special_tokens=True)[0])\n",
    "\n",
    "generated_outs = model.generate(**model_inputs,\n",
    "                                do_sample=True,\n",
    "                                max_new_tokens=30)\n",
    "print('creative out')\n",
    "print(tokenizer.batch_decode(generated_outs, skip_special_tokens=True)[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "511002e5-7479-46b7-82c4-1c6e95724ae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kamal/jupyter_env/lib/python3.11/site-packages/transformers/generation/utils.py:1539: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21\n"
     ]
    }
   ],
   "source": [
    "# decoder only archs continue to iterate over your input prompts. \n",
    "# reason for left padding is to llm are not trained to continue from pad tokens\n",
    "\n",
    "mod_ins = tokenizer(\n",
    "    ['1, 2, 3', 'a, b, k, d, e'], padding=True, return_tensors=\"pt\"\n",
    ")\n",
    "gen_outs = model.generate(**mod_ins, max_new_tokens=36)\n",
    "\n",
    "print(tokenizer.batch_decode(gen_outs, skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "797898a1-20e7-4ac9-bc94-0a5ebae33b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a friendly chatbot who always responds in the style of a thug\",\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"How many helicopters can a human eat in one sitting?\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e96547fe-fc95-4895-9327-9953c6632993",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using sep_token, but it is not set yet.\n",
      "Using cls_token, but it is not set yet.\n",
      "Using mask_token, but it is not set yet.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_chats = tokenizer.apply_chat_template(messages,\n",
    "                                            # add_generation_prompt=True,\n",
    "                                            return_tensors='pt',).to('cuda')\n",
    "in_length = model_chats.shape[1]\n",
    "in_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1be8943b-dc8c-47bd-94a4-d81bd308330d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Djokovic, Novak to defend Paris opener\n",
      "Serbian Open tennis star Novak Djokovic made a big statement before his Paris Open debut, defeating France's Laurent Grosjean 6-3 4-6 6-4,\n"
     ]
    }
   ],
   "source": [
    "gen_ids = model.generate(model_chats, do_sample=True, max_new_tokens=50)\n",
    "print(tokenizer.batch_decode(gen_ids[:, in_length:], skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8672d5ca-1ce6-4ecd-beb9-815e9e54cb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# continue with the text generation strategies\n",
    "# https://huggingface.co/docs/transformers/generation_strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1eb7f4-4dee-4837-ab0a-80e2652a069b",
   "metadata": {},
   "source": [
    "This guide describe\r\n",
    "\n",
    "\r\n",
    "default generation conf\n",
    "    \n",
    "    - The default generation configuration limits the size of the output combined with the input prompt to a maximum of 20 tokens to avoid running into resource limitations. \n",
    "    \n",
    "    - The default decoding strategy is greedy search, which is the simplest decoding strategy that picks a token with the highest probability as the next token. \n",
    "    \n",
    "    - For many tasks and small output sizes this works well\n",
    "\n",
    "n\r\n",
    "common decoding strategies and their main paramet\n",
    "\n",
    "rs\r\n",
    "saving and sharing custom generation configurations with your fine-tuned model on ðŸ¤— Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2810df94-cffc-47d6-bb69-ff11bed1a550",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerationConfig {\n",
       "  \"bos_token_id\": 2,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"pad_token_id\": 1\n",
       "}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generation_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fdfe7c5-01d0-4e28-b072-33759535bb03",
   "metadata": {},
   "source": [
    "## Four params of generate method\n",
    "\n",
    "**max_new_tokens:** the maximum number of tokens to generate. Does not including the tokens in the prompt. Alternative to stopping criteria\r\n",
    "**\n",
    "num_beams**: by specifying a number of beams higher than 1, you are effectively switching from greedy search to beam search. This has the** advantage of identifying high-probability sequence**s that start with a lower probability initial tokens and wouldâ€™ve been ignored by the greedy searc\n",
    "\n",
    ".\r\n",
    "do_sample: if set to True, this parameter enables decoding strategies such \n",
    "\n",
    "    - s multinomial sampling\n",
    "    \n",
    "    - , beam-search multinomial sampling\n",
    "    \n",
    "    - , Top-K sampling a\n",
    "    \n",
    "    - d Top-p samplig\n",
    "\n",
    "s.\r\n",
    "num_return_sequences: the number of sequence candidates to return for each input. This option is only available for the decoding strategies that support multiple sequence candidates, e.g. variations of beam search and samplin\n",
    "g. Decoding strategies like greedy search and contrastive search return a single output sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "02ba771b-c88e-4bf4-9b90-5aa32715000e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GenerationConfig\n",
    "\n",
    "generation_config = GenerationConfig(\n",
    "    max_new_tokens=50,\n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    eos_token_id=model.config.eos_token_id\n",
    ")\n",
    "\n",
    "generation_config.save_pretrained(\"/home/kamal/training_files/generation_config\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8ffd73f6-1269-49f2-9bd2-42e752ee4510",
   "metadata": {},
   "outputs": [],
   "source": [
    "translation_generation_config = GenerationConfig(\n",
    "    num_beams=4,\n",
    "    early_stopping=True,\n",
    "    decoder_start_token_id=0,\n",
    "    eos_token_id=model.config.eos_token_id,\n",
    "    pad_token=model.config.pad_token_id,\n",
    ")\n",
    "\n",
    "translation_generation_config.save_pretrained(\"/home/kamal/training_files/translation_config\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "73599258-9b3b-4e2c-9d21-703e51c503a2",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "/home/kamal/jupyter_env/lib/python3.11/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/kamal/jupyter_env/lib/python3.11/site-packages/transformers/generation/utils.py:1539: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "\"log_softmax_lastdim_kernel_impl\" not implemented for 'Half'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtranslate English to French: Configuration files are easy to use!\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m      2\u001b[0m                    return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtranslation_generation_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(tokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(outputs, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n",
      "File \u001b[0;32m~/jupyter_env/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/jupyter_env/lib/python3.11/site-packages/transformers/generation/utils.py:1685\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1678\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1679\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1680\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   1681\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1682\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1683\u001b[0m     )\n\u001b[1;32m   1684\u001b[0m     \u001b[38;5;66;03m# 13. run beam search\u001b[39;00m\n\u001b[0;32m-> 1685\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbeam_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1686\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1687\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeam_scorer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1688\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1689\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1690\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1691\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1692\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1693\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1694\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1695\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1696\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1698\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE:\n\u001b[1;32m   1699\u001b[0m     \u001b[38;5;66;03m# 11. prepare logits warper\u001b[39;00m\n\u001b[1;32m   1700\u001b[0m     logits_warper \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_logits_warper(generation_config)\n",
      "File \u001b[0;32m~/jupyter_env/lib/python3.11/site-packages/transformers/generation/utils.py:3036\u001b[0m, in \u001b[0;36mGenerationMixin.beam_search\u001b[0;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   3033\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n\u001b[1;32m   3035\u001b[0m next_token_logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\n\u001b[0;32m-> 3036\u001b[0m next_token_scores \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_softmax\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3037\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnext_token_logits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m   3038\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (batch_size * num_beams, vocab_size)\u001b[39;00m\n\u001b[1;32m   3040\u001b[0m next_token_scores_processed \u001b[38;5;241m=\u001b[39m logits_processor(input_ids, next_token_scores)\n\u001b[1;32m   3041\u001b[0m next_token_scores \u001b[38;5;241m=\u001b[39m next_token_scores_processed \u001b[38;5;241m+\u001b[39m beam_scores[:, \u001b[38;5;28;01mNone\u001b[39;00m]\u001b[38;5;241m.\u001b[39mexpand_as(\n\u001b[1;32m   3042\u001b[0m     next_token_scores_processed\n\u001b[1;32m   3043\u001b[0m )\n",
      "File \u001b[0;32m~/jupyter_env/lib/python3.11/site-packages/torch/nn/functional.py:1932\u001b[0m, in \u001b[0;36mlog_softmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   1930\u001b[0m     dim \u001b[38;5;241m=\u001b[39m _get_softmax_dim(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlog_softmax\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim(), _stacklevel)\n\u001b[1;32m   1931\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1932\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_softmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1933\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1934\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mlog_softmax(dim, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: \"log_softmax_lastdim_kernel_impl\" not implemented for 'Half'"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(\"translate English to French: Configuration files are easy to use!\", \n",
    "                   return_tensors=\"pt\")\n",
    "\n",
    "outputs = model.generate(**inputs,\n",
    "                         generation_config=translation_generation_config)\n",
    "# need to use a different model, nt opt-350m\n",
    "print(tokenizer.batch_decode(outputs, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5fb32f39-7143-4e01-bccf-92a13a9c1764",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An incrementing seq: one of the following:\n",
      "\n",
      "1. The first element of the seq is the first element of the seq.\n",
      "\n",
      "2. The second element of the seq is the second element of the seq.\n",
      "\n",
      "3. The third element of the seq is the third element of the seq.\n",
      "\n",
      "4. The fourth element of the seq is the fourth element of the seq.\n",
      "\n",
      "5. The fifth element of the seq is the fifth element of the seq.\n",
      "\n",
      "6. The sixth\n"
     ]
    }
   ],
   "source": [
    "from transformers import TextStreamer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "inputs = tok([\"An incrementing seq: one\"], return_tensors='pt')\n",
    "\n",
    "streamer = TextStreamer(tok)\n",
    "\n",
    "_ = model.generate(**inputs, streamer=streamer, max_new_tokens=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3381ffb6-414b-4e6c-83a1-9de83498cf1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[' illicit leftpr Ifu main west in m newsr Sou main west for nationalr Sou main west for nationalr Sou main west for nationalr Sou main west for nationalr Sou main west for nationalr Sou main west for nationalr Sou main west for nationalr Sou main west for nationalr Sou main west for nationalr Sou main west for nationalr Sou main west for nationalr Sou main west for nationalr Sou main west for nationalr Sou']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate uses greedy search decoding by default so you donâ€™t have to pass any parameters to enable \n",
    "\n",
    "# two main parameters that enable and control the behavior of contrastive search \n",
    "# are penalty_alpha and top_k:\n",
    "\n",
    "inputs = tok([\"Apple Iphone is\"], return_tensors='pt')\n",
    "\n",
    "outputs = model.generate(**inputs,\n",
    "                         penalty_alpha=0.6,\n",
    "                         top_k=4,\n",
    "                         max_new_tokens=100)\n",
    "\n",
    "tokenizer.batch_decode(outputs, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c728ca5-6eb4-4f04-bdea-c64272ae2533",
   "metadata": {},
   "source": [
    "As opposed to greedy search that always chooses a token with the highest probability as the next token, multinomial sampling (also called ancestral sampling) randomly selects the next token based on the probability distribution over the entire vocabulary given by the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bb71250b-0859-425d-afa9-2fe3629bc5f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['. These design principles allow us to design the most efficient and sustainable human habitation and food production systems...... are. that can be applied to any location, climate and culture.. and and, combines and reflects the best of these disciplines. and.... Each design principle itself is a complete conceptual framework based on sound scientific principles...']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model.generate(**inputs,\n",
    "                         do_sample=True,\n",
    "                         num_beams=1,\n",
    "                         max_new_tokens=100,\n",
    "                        temperature=0.3)\n",
    "\n",
    "tokenizer.batch_decode(outputs, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6a0b1ac8-bebb-4d54-909d-54793f11afb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['. Each design principle embodies a set of universal design principles that can be applied to any location, climate and culture..... are a set of universal design principles... that can be applied to any.. and, and and. The design principles are a set of universal design principles that.. design principles that are. into one.']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In Top-K sampling, the K most likely next words are filtered and the probability mass is \n",
    "# redistributed among only those K next words. \n",
    "outputs = model.generate(**inputs,\n",
    "                         do_sample=True,\n",
    "                         num_beams=1,\n",
    "                         max_new_tokens=100,\n",
    "                        temperature=0.3,\n",
    "                        top_k=5)\n",
    "\n",
    "tokenizer.batch_decode(outputs, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e4e34193-b8d2-42c0-9f8d-ae8d5cf9c089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      ". The Permaculture Design Principles are a set of. We call these a â€œPlant Systemâ€ because they are all parts of a plant, and every element is a â€œplantâ€..\n"
     ]
    }
   ],
   "source": [
    "#  Top-p sampling chooses from the smallest possible set of \n",
    "# words whose cumulative probability exceeds the probability p.\n",
    "# deactivate top_k sampling and sample only from 92% most likely words\n",
    "sample_output = model.generate(\n",
    "    **inputs, \n",
    "    do_sample=True, \n",
    "    max_length=50, \n",
    "    top_p=0.92, \n",
    "    top_k=0\n",
    ")\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "501d945f-12b8-4cb5-a79b-8282b76c2e59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' illicit leftpr If claimedz morning- Jordanum for around around learn There laterF industrial never stronger in04 detailsÃ³ family supported water for around around learn There laterF industrial never stronger in04 detailsÃ³ family supported water for target There laterF industrial never stronger in04']\n"
     ]
    }
   ],
   "source": [
    "# Unlike greedy search, beam-search decoding keeps several hypotheses at each time step and eventually \n",
    "# chooses the hypothesis that has the overall highest probability for the entire sequence.\n",
    "\n",
    "outputs = model.generate(**inputs,\n",
    "                         num_beams=5,\n",
    "                         max_new_tokens=50)\n",
    "\n",
    "print(tokenizer.batch_decode(outputs, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b3bf92b1-bbbc-4043-968d-c1ff206bd66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "checkpoint = \"t5-large\"\n",
    "prompt = \"It is astonishing how one can\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to('cuda')\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint).to('cuda')\n",
    "\n",
    "outputs = model.generate(**inputs,\n",
    "                         num_beams=3,\n",
    "                         do_sample=True,\n",
    "                         max_new_tokens=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d77886-3684-4181-9c0b-83318230bcd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_output = model.generate(\n",
    "    **inputs, \n",
    "    max_length=50, \n",
    "    num_beams=5, \n",
    "    no_repeat_ngram_size=2, \n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(beam_output[0], skip_special_tokens=True))\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb8b842-76b9-4785-a729-6b0d1d50207d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set return_num_sequences > 1 and num_return_sequence <= num_beams\n",
    "beam_outputs = model.generate(\n",
    "    input_ids, \n",
    "    max_length=50, \n",
    "    num_beams=5, \n",
    "    no_repeat_ngram_size=2, \n",
    "    num_return_sequences=5, \n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "for i, beam_output in enumerate(beam_outputs):\n",
    "  print(\"{}: {}\".format(i, tokenizer.decode(beam_output, skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "82d7978e-313f-47b2-ad67-be25bb5cdaec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0, 32099,   103,    24,     5,     1]], device='cuda:0')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "02a7b123-79d0-434f-9fc3-2cfc072d0ffc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'. The Permaculture Design Principles are a set of universal design principles that can be applied to any location, climate and culture.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = (\n",
    "    \"The Permaculture Design Principles are a set of universal design principles \"\n",
    "    \"that can be applied to any location, climate and culture, and they allow us to design \"\n",
    "    \"the most efficient and sustainable human habitation and food production systems. \"\n",
    "    \"Permaculture is a design system that encompasses a wide variety of disciplines, such \"\n",
    "    \"as ecology, landscape design, environmental science and energy conservation, and the \"\n",
    "    \"Permaculture design principles are drawn from these various disciplines. Each individual \"\n",
    "    \"design principle itself embodies a complete conceptual framework based on sound \"\n",
    "    \"scientific principles. When we bring all these separate  principles together, we can \"\n",
    "    \"create a design system that both looks at whole systems, the parts that these systems \"\n",
    "    \"consist of, and how those parts interact with each other to create a complex, dynamic, \"\n",
    "    \"living system. Each design principle serves as a tool that allows us to integrate all \"\n",
    "    \"the separate parts of a design, referred to as elements, into a functional, synergistic, \"\n",
    "    \"whole system, where the elements harmoniously interact and work together in the most \"\n",
    "    \"efficient way possible.\"\n",
    ")\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to('cuda')\n",
    "\n",
    "outputs = model.generate(**inputs,\n",
    "                         num_beams=5,\n",
    "                         num_beam_groups=5,\n",
    "                         max_new_tokens=30,\n",
    "                         diversity_penalty=1.0)\n",
    "\n",
    "tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8be0fcf5-5bdf-40b9-9a89-0ab8dc9b5fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0: , or to. The Permaculture Design Principles are a set of universal design principles.. The Permaculture Design Principles are a set of universal design principles that can be applied to any location\n",
      "1: . Each design principle is used to help us create systems that are effective and sustainable. The Permaculture Design Principles are a set of universal design principles that can be applied to any location, climate and culture..\n",
      "2: . Each design principle serves as a tool that allows us to integrate all the separate parts of a design, referred to as elements, into a functional, dynamic, whole system.....\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# set seed to reproduce results. Feel free to change the seed though to get different results\n",
    "\n",
    "# set top_k = 50 and set top_p = 0.95 and num_return_sequences = 3\n",
    "sample_outputs = model.generate(\n",
    "    **inputs,\n",
    "    do_sample=True, \n",
    "    max_length=50, \n",
    "    top_k=50, \n",
    "    top_p=0.95, \n",
    "    num_return_sequences=3\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "for i, sample_output in enumerate(sample_outputs):\n",
    "  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e759c5-c768-4758-bc3d-b94175c75ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed\n",
    "set_seed(42)  # For reproducibility\n",
    "\n",
    "prompt = \"Alice and Bob\"\n",
    "checkpoint = \"EleutherAI/pythia-1.4b-deduped\"\n",
    "assistant_checkpoint = \"EleutherAI/pythia-160m-deduped\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint)\n",
    "assistant_model = AutoModelForCausalLM.from_pretrained(assistant_checkpoint)\n",
    "outputs = model.generate(**inputs, assistant_model=assistant_model, do_sample=True, temperature=0.5)\n",
    "tokenizer.batch_decode(outputs, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3774c72f-b870-48bb-acd3-5f6087213b8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
